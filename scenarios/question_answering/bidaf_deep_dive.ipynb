{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import json\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "#import utils\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "\n",
    "from azureml.core import Datastore, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDAF Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf_settings = {\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"squad\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": True\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"character_tokenizer\": {\n",
    "          \"byte_encoding\": \"utf-8\",\n",
    "          \"start_tokens\": [259],\n",
    "          \"end_tokens\": [260]\n",
    "        },\n",
    "        \"min_padding_length\": 5\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"https://allennlp.s3.amazonaws.com/datasets/squad/squad-train-v1.1.json\",\n",
    "  \"validation_data_path\": \"https://allennlp.s3.amazonaws.com/datasets/squad/squad-dev-v1.1.json\",\n",
    "  \"evaluate_on_test\": True,\n",
    "  \"model\": {\n",
    "    \"type\": \"bidaf\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "        \"tokens\": {\n",
    "          \"type\": \"embedding\",\n",
    "          \"pretrained_file\": \"https://allennlp.s3.amazonaws.com/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "          \"embedding_dim\": 100,\n",
    "          \"trainable\": False\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "          \"type\": \"character_encoding\",\n",
    "          \"embedding\": {\n",
    "            \"num_embeddings\": 262,\n",
    "            \"embedding_dim\": 16\n",
    "          },\n",
    "          \"encoder\": {\n",
    "            \"type\": \"cnn\",\n",
    "            \"embedding_dim\": 16,\n",
    "            \"num_filters\": 100,\n",
    "            \"ngram_filter_sizes\": [5]\n",
    "          },\n",
    "          \"dropout\": 0.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"num_highway_layers\": 2,\n",
    "    \"phrase_layer\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": True,\n",
    "      \"input_size\": 200,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"similarity_function\": {\n",
    "      \"type\": \"linear\",\n",
    "      \"combination\": \"x,y,x*y\",\n",
    "      \"tensor_1_dim\": 200,\n",
    "      \"tensor_2_dim\": 200\n",
    "    },\n",
    "    \"modeling_layer\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": True,\n",
    "      \"input_size\": 800,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.2\n",
    "    },\n",
    "    \"span_end_encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": True,\n",
    "      \"input_size\": 1400,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"dropout\": 0.2\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"bucket\",\n",
    "    \"sorting_keys\": [[\"passage\", \"num_tokens\"], [\"question\", \"num_tokens\"]],\n",
    "    \"batch_size\": 40\n",
    "  },\n",
    "\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": 1, #20\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 10,\n",
    "    \"validation_metric\": \"+em\",\n",
    "    \"cuda_device\": 0,\n",
    "    \"learning_rate_scheduler\": {\n",
    "      \"type\": \"reduce_on_plateau\",\n",
    "      \"factor\": 0.5,\n",
    "      \"mode\": \"max\",\n",
    "      \"patience\": 2\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"betas\": [0.9, 0.9]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"squad\", exist_ok=True)\n",
    "\n",
    "with open(\"squad/bidaf_config.json\", \"w\") as f:\n",
    "    f.write(json.dumps(bidaf_settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AzureML Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the necessary components for running this as an AzureML experiment\n",
    "1. Create or link to an existing `Workspace`\n",
    "2. Set up an `Experiment` with `logging`\n",
    "3. Create or attach existing `AmlCompute`\n",
    "4. Upload our data to a `Datastore`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to or create a Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, go through the [Configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`. This will create a config.json file containing the values needed below to create a workspace.\n",
    "\n",
    "**Note**: you do not need to fill in these values if you have a config.json in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MAIDAPTest\n",
      "Azure region: eastus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up an Experiment and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a folder for the project\n",
    "project_folder = \"./bidaf-question-answering\"\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Set up an experiment\n",
    "experiment_name = \"bidaf-question-answering\"\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "# Add logging to our experiment\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link AmlCompute Compute Target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to link a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training our model (see [compute options](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#supported-compute-targets) for explanation of the different options). We will use an [AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) target and link to an existing target (if the cluster_name exists) or create a STANDARD_NC6 GPU cluster (autoscales from 0 to 4 nodes) in this example. Creating a new AmlComputes takes approximately 5 minutes. \n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-06-26T01:16:30.014000+00:00', 'errors': None, 'creationTime': '2019-05-20T22:09:40.142683+00:00', 'modifiedTime': '2019-05-20T22:10:11.888950+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# choose your cluster\n",
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=4\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute.\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uploads our local data to a `Datastore` so that the data is accessible from the remote compute target and creates a `DataReference` to point to the location of the data on the Datastore. A DataStore is backed either by a Azure File Storage (default option) or Azure Blob Storage ([how to decide between these options](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks)) and data is made accessible by mounting or copying data to the compute target. `ws.datastores` lists all options for datastores and `ds.account_name` gets the name of the datastore that can be used to find it in the Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('squad/squad_dev.json', <http.client.HTTPMessage at 0x2c70027bb38>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlretrieve(\n",
    "    \"https://allennlp.s3.amazonaws.com/datasets/squad/squad-train-v1.1.json\",\n",
    "    filename=\"squad/squad_train.json\",\n",
    ")\n",
    "\n",
    "urlretrieve(\n",
    "    \"https://allennlp.s3.amazonaws.com/datasets/squad/squad-dev-v1.1.json\",\n",
    "    filename=\"squad/squad_dev.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./squad\\bidaf_config.json\n",
      "Uploading ./squad\\squad_dev.json\n",
      "Uploading ./squad\\squad_train.json\n",
      "Uploaded ./squad\\bidaf_config.json, 1 files out of an estimated total of 3\n",
      "Uploaded ./squad\\squad_dev.json, 2 files out of an estimated total of 3\n",
      "Uploaded ./squad\\squad_train.json, 3 files out of an estimated total of 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_0583283c61a1402cbdf6b6d13ee970e5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a specific datastore or you can call ws.get_default_datastore()\n",
    "datastore_name = \"workspacefilestore\"\n",
    "ds = ws.datastores[datastore_name]\n",
    "\n",
    "# Upload files in data folder to the datastore\n",
    "ds.upload(\n",
    "    src_dir=\"./squad\",\n",
    "    target_path=\"squad_data\",\n",
    "    overwrite=True,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bidaf-question-answering/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/train.py\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from allennlp.common import Params\n",
    "from allennlp.commands.train import train_model\n",
    "\n",
    "def load_params(folder, file):\n",
    "    with open(os.path.join(folder, file)) as f:\n",
    "        param_dict = json.load(f)\n",
    "    return Params(param_dict)\n",
    "\n",
    "def main():\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    # get command-line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_folder', type=str, \n",
    "                        help='Folder where data is stored')\n",
    "    parser.add_argument('--config_name', type=str, \n",
    "                        help='Name of json configuration file')\n",
    "    args = parser.parse_args()\n",
    "    squad_folder = os.path.join(args.data_folder, \"squad_data\")\n",
    "    \n",
    "    params = load_params(squad_folder, args.config_name)\n",
    "    \n",
    "    train_model(params,\n",
    "           serialization_dir = os.path.join(squad_folder, \"logs\"),\n",
    "           file_friendly_logging = True,\n",
    "           recover = False,\n",
    "           force = True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch Estimator\n",
    "The Azure ML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data_folder': ds.as_mount(),\n",
    "    '--config_name': 'bidaf_config.json'}\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder,\n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script= \"train.py\",\n",
    "                    use_gpu=True,\n",
    "                    conda_dependencies_file_path=\"bidafenv.yml\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: bidaf-question-answering,\n",
      "Id: bidaf-question-answering_1562078057_9d9e44ab,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Starting)\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654c3e5e1ecc43c491cdde30f5638950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading squad_data\\logs\\vocabulary\\tokens.txt\n",
      "Downloading squad_data\\logs\\vocabulary\\non_padded_namespaces.txt\n",
      "Downloading squad_data\\logs\\log\\validation\\events.out.tfevents.1562078333.3e6b26e42d944b088e5c10bc5811cb6d000000\n",
      "Downloading squad_data\\logs\\log\\train\\events.out.tfevents.1562078333.3e6b26e42d944b088e5c10bc5811cb6d000000\n",
      "Downloading squad_data\\logs\\training_state_epoch_0.th\n",
      "Downloading squad_data\\logs\\stdout.log\n",
      "Downloading squad_data\\logs\\stderr.log\n",
      "Downloading squad_data\\logs\\model_state_epoch_0.th\n",
      "Downloading squad_data\\logs\\model.tar.gz\n",
      "Downloading squad_data\\logs\\metrics.json\n",
      "Downloading squad_data\\logs\\metrics_epoch_0.json\n",
      "Downloading squad_data\\logs\\best.th\n",
      "Downloading squad_data\\logs\\config.json\n",
      "Downloading squad_data\\squad_train.json\n",
      "Downloading squad_data\\squad_dev.json\n",
      "Downloading squad_data\\bidaf_config.json\n",
      "Downloaded squad_data\\logs\\log\\validation\\events.out.tfevents.1562078333.3e6b26e42d944b088e5c10bc5811cb6d000000, 1 files out of an estimated total of 21\n",
      "Downloaded squad_data\\logs\\stdout.log, 2 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\vocabulary\\non_padded_namespaces.txt, 3 files out of an estimated total of 18\n",
      "Downloaded squad_data\\logs\\vocabulary\\tokens.txt, 4 files out of an estimated total of 18\n",
      "Downloaded squad_data\\logs\\metrics_epoch_0.json, 5 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\config.json, 6 files out of an estimated total of 22\n",
      "Downloaded squad_data\\bidaf_config.json, 7 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\metrics.json, 8 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\stderr.log, 9 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\log\\train\\events.out.tfevents.1562078333.3e6b26e42d944b088e5c10bc5811cb6d000000, 10 files out of an estimated total of 22\n",
      "Downloaded squad_data\\squad_dev.json, 11 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\training_state_epoch_0.th, 12 files out of an estimated total of 22\n",
      "Downloaded squad_data\\squad_train.json, 13 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\model.tar.gz, 14 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\best.th, 15 files out of an estimated total of 22\n",
      "Downloaded squad_data\\logs\\model_state_epoch_0.th, 16 files out of an estimated total of 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.download(target_path=\"DownloadedLogs\",\n",
    "            prefix='squad_data',\n",
    "            show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
