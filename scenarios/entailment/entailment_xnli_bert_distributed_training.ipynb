{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference on MultiNLI Dataset using BERT with Azure Machine Learning\n",
    "\n",
    "## Summary\n",
    "In this notebook, we demostrate using the BERT model to do language inference in English. We use the [XNLI](https://github.com/facebookresearch/XNLI) dataset and the task is to classify sentence pairs into three classes: contradiction, entailment, and neutral.   \n",
    "The figure below shows how [BERT](https://arxiv.org/abs/1810.04805) classifies sentence pairs. It concatenates the tokens in each sentence pairs and separates the sentences by the [SEP] token. A [CLS] token is prepended to the token list and used as the aggregate sequence representation for the classification task.\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_two_sentence.PNG\">\n",
    "\n",
    "Azure Machine Learning features higlighted in the notebook : \n",
    "\n",
    "- Distributed training with Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "Azure ML SDK Version: 1.0.48\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import azureml.core\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AzureML Setup\n",
    "\n",
    "### 2.1 Link to or create a Workspace\n",
    "\n",
    "First, go through the [Configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`. This will create a config.json file containing the values needed below to create a workspace.\n",
    "\n",
    "**Note**: you do not need to fill in these values if you have a config.json in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAP-Entailment\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MAIDAP-Entailment\n",
      "Azure region: eastus\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Link AmlCompute Compute Target\n",
    "\n",
    "We need to link a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training our model (see [compute options](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#supported-compute-targets) for explanation of the different options). We will use an [AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) target and link to an existing target (if the cluster_name exists) or create a STANDARD_NC6 GPU cluster (autoscales from 0 to 4 nodes) in this example. Creating a new AmlComputes takes approximately 5 minutes. \n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: gpu-entail\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-07-31T22:29:25.780000+00:00', 'errors': None, 'creationTime': '2019-07-27T02:14:46.127092+00:00', 'modifiedTime': '2019-07-27T02:15:07.181277+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6S_V2'}\n"
     ]
    }
   ],
   "source": [
    "#cluster_name = \"bertncrs24\"\n",
    "cluster_name = \"gpu-entail\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found compute target: {}\".format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(cluster_name))\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=1\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./entail_utils\\\\utils_nlp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = True\n",
    "project_dir = \"./entail_utils\"\n",
    "if DEBUG and os.path.exists(project_dir): \n",
    "    shutil.rmtree(project_dir) \n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(project_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./entail_utils/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_dir/train.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import horovod.torch as hvd\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from utils_nlp.dataset.xnli import load_pandas_df\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.models.bert.sequence_classification_distributed import (\n",
    "    BERTSequenceDistClassifier,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# constants to use in the file\n",
    "TRAIN_FILE_SPLIT = \"train\"\n",
    "TEST_FILE_SPLIT = \"test\"\n",
    "TO_LOWER = True\n",
    "LANGUAGE_ENGLISH = Language.ENGLISH\n",
    "MAX_SEQ_LENGTH = 128\n",
    "LABEL_COL = \"label\"\n",
    "TEXT_COL = \"text\"\n",
    "TRAIN_DATA_USED_PERCENT = 0.0025\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "NUM_GPUS = 1\n",
    "\n",
    "## each machine gets it's own copy of data\n",
    "CACHE_DIR = \"./xnli-data-%d\" % hvd.rank()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--seed\",\n",
    "    type=int,\n",
    "    default=42,\n",
    "    metavar=\"S\",\n",
    "    help=\"random seed (default: 42)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-cuda\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"disables CUDA training\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(args.cuda)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "train_df = load_pandas_df(\n",
    "    local_cache_path=CACHE_DIR, file_split=\"train\", language=\"en\"\n",
    ")\n",
    "train_data_used_count = round(TRAIN_DATA_USED_PERCENT * train_df.shape[0])\n",
    "train_df = train_df.loc[:train_data_used_count]\n",
    "\n",
    "dev_df = load_pandas_df(\n",
    "    local_cache_path=CACHE_DIR, file_split=\"dev\", language=\"en\"\n",
    ")\n",
    "test_df = load_pandas_df(\n",
    "    local_cache_path=CACHE_DIR, file_split=\"test\", language=\"en\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(LANGUAGE_ENGLISH, to_lower=TO_LOWER, cache_dir=CACHE_DIR)\n",
    "train_tokens = tokenizer.tokenize(train_df[TEXT_COL])\n",
    "test_tokens = tokenizer.tokenize(test_df[TEXT_COL])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df[LABEL_COL])\n",
    "num_labels = len(np.unique(train_labels))\n",
    "\n",
    "train_token_ids, train_input_mask, train_token_type_ids = tokenizer.preprocess_classification_tokens(\n",
    "    train_tokens, max_len=MAX_SEQ_LENGTH\n",
    ")\n",
    "\n",
    "classifier = BERTSequenceDistClassifier(\n",
    "    language=LANGUAGE_ENGLISH, num_labels=num_labels, cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "classifier.fit(\n",
    "    token_ids=train_token_ids,\n",
    "    input_mask=train_input_mask,\n",
    "    token_type_ids=train_token_type_ids,\n",
    "    labels=train_labels,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    warmup_proportion=WARMUP_PROPORTION,\n",
    ")\n",
    "\n",
    "\n",
    "# evaluation\n",
    "if hvd.rank() == 0:\n",
    "    NUM_GPUS = 1\n",
    "    kwargs = {}\n",
    "\n",
    "    test_token_ids, test_input_mask, test_token_type_ids = tokenizer.preprocess_classification_tokens(\n",
    "        test_tokens, max_len=MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "    predictions, labels = classifier.predict(\n",
    "        token_ids=test_token_ids,\n",
    "        input_mask=test_input_mask,\n",
    "        token_type_ids=test_token_type_ids,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    predictions = label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    print(classification_report(test_df[LABEL_COL], predictions.tolist(), digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a PyTorch Estimator\n",
    "\n",
    "BERT is built on PyTorch, so we will use the AzureML SDK's PyTorch estimator to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, see [How to Train Pytorch Models on AzureML](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). First we set up a .yml file with the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_COUNT = 4\n",
    "mpiConfig=MpiConfiguration()\n",
    "mpiConfig.process_count_per_node=1\n",
    "\n",
    "est = PyTorch(\n",
    "    source_directory=project_dir,\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"train.py\",\n",
    "    node_count=NODE_COUNT,\n",
    "    distributed_training=mpiConfig,\n",
    "    use_gpu=True,\n",
    "    framework_version=\"1.0\",\n",
    "    conda_packages=[\"scikit-learn=0.20.3\", \"numpy\", \"spacy\", \"nltk\"],\n",
    "    pip_packages=[\"pandas\",\"seqeval[gpu]\", \"pytorch-pretrained-bert\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Experiment and Submit a Job\n",
    "Submit the estimator object to run your experiment. Results can be monitored using a Jupyter widget. The widget and run are asynchronous and update every 10-15 seconds until job completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, name=\"Nlp-Entailment-BERT\")\n",
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe8f4c51faf473093d5d49cfd72b8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
