{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import azureml.core\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAIPBERT-eastus\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAIPBERT-eastus\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MAIDAIPBERT-eastus\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "print(\"Workspace name: {}\".format(ws.name))\n",
    "print(\"Resource group: {}\".format(ws.resource_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"bertncrs24\"\n",
    "#cluster_name = \"gpu-entail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: bertncrs24\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-07-29T19:42:36.790000+00:00', 'errors': None, 'creationTime': '2019-07-12T19:59:45.933132+00:00', 'modifiedTime': '2019-07-12T20:00:01.793458+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC24RS_V3'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found compute target: {}\".format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(cluster_name))\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=1\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./entail_utils\\\\utils_nlp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = True\n",
    "project_dir = \"./entail_utils\"\n",
    "if DEBUG and os.path.exists(project_dir): \n",
    "    shutil.rmtree(project_dir) \n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(project_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./entail_utils/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_dir/train.py\n",
    "\n",
    "import horovod.torch as hvd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from utils_nlp.dataset.xnli_torch_dataset import XnliDataset\n",
    "from utils_nlp.models.bert.common import Language\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from utils_nlp.models.bert.sequence_classification_distributed import BERTSequenceClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from azureml.core.run import Run\n",
    "# get the Azure ML run object\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "LANGUAGE_ENGLISH = \"en\"\n",
    "#CACHE_DIR = \"./xnli_data3\"\n",
    "TRAIN_FILE_SPLIT = \"train\"\n",
    "TEST_FILE_SPLIT = \"test\"\n",
    "TO_LOWERCASE = True \n",
    "PRETRAINED_BERT_LNG = Language.ENGLISH\n",
    "\n",
    "# optimizer configurations\n",
    "LEARNING_RATE= 5e-5\n",
    "WARMUP_PROPORTION= 0.1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "NUM_GPUS = 4\n",
    "\n",
    "## each machine gets it's own copy of data\n",
    "CACHE_DIR = './xnli-data-%d' % hvd.rank()\n",
    "print(\"======= cache dir =======================\", CACHE_DIR)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Training settings\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',help='random seed (default: 42)')\n",
    "parser.add_argument('--epochs', type=int, default=2, metavar='N', help='number of epochs to train (default: 2)')\n",
    "parser.add_argument('--num_workers', type=int, default=2, metavar='N', help='number of workers to train (default: 2)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,help='disables CUDA training')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(args.cuda)\n",
    "\n",
    "'''\n",
    "For example, you have 4 nodes and 4 GPUs each node, so you spawn 16 workers. \n",
    "Every worker will have a rank [0, 15], and every worker will have a local_rank [0, 3].\n",
    "You use local_rank for GPU pinning because there's typically one GPU available on the node per process. \n",
    "It wouldn't make sense to use rank here because rank could be 10, but you only have 4 GPUs so there is no GPU 10.\n",
    "In the MNIST example, \n",
    "we give each worker a separate copy of the data and give it a unique name based on the rank of the worker \n",
    "that will be using that data. If all the data is being copied to local disk, then you could use local_rank here, \n",
    "but often we use a shared filesystem, so if you used local_rank, processes would be overwriting \n",
    "each other's data during the download process.\n",
    "'''\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "#kwargs = {}\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_dataset = XnliDataset(file_split=TRAIN_FILE_SPLIT, \n",
    "                            cache_dir=CACHE_DIR, \n",
    "                            language=LANGUAGE_ENGLISH,\n",
    "                            to_lowercase=TO_LOWERCASE,\n",
    "                            tok_language=PRETRAINED_BERT_LNG)\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "train_loader =  DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, **kwargs)\n",
    "    \n",
    "#set the label_encoder for evaluation datset\n",
    "label_encoder = train_dataset.label_encoder\n",
    "num_labels = len(np.unique(train_dataset.labels))\n",
    "\n",
    "classifier = BERTSequenceClassifier(language=PRETRAINED_BERT_LNG,\n",
    "                                            num_labels=num_labels,\n",
    "                                            cache_dir=CACHE_DIR,\n",
    "                                            )\n",
    "\n",
    "# optimizer configurations\n",
    "num_samples = len(train_loader.dataset)\n",
    "num_batches = int(num_samples/BATCH_SIZE)\n",
    "num_workers = args.num_workers\n",
    "num_train_optimization_steps = num_batches*args.epochs #int(num_batches/hvd.size()) * args.epochs \n",
    "optimizer_grouped_parameters = classifier.optimizer_params\n",
    "\n",
    "print(\"================= num_train_optimization_steps ==============================\")\n",
    "print(num_train_optimization_steps)\n",
    "\n",
    "lr=LEARNING_RATE * hvd.size()\n",
    "\n",
    "bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                   lr=lr,\n",
    "                   t_total=num_train_optimization_steps,\n",
    "                   warmup=WARMUP_PROPORTION,)\n",
    "\n",
    "if WARMUP_PROPORTION is None:\n",
    "    print(\"================== Without Warmup proprtion ===========================\")\n",
    "    bert_optimizer = BertAdam(optimizer_grouped_parameters, lr=lr)\n",
    "else:\n",
    "    print(\"================== With Warmup proportion =============================\")\n",
    "    bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                   lr=lr,\n",
    "                   t_total=num_train_optimization_steps,\n",
    "                   warmup=WARMUP_PROPORTION,\n",
    "                  )\n",
    "\n",
    "\n",
    "## Distributed optimizer\n",
    "bert_optimizer = hvd.DistributedOptimizer(bert_optimizer, classifier.model.named_parameters())\n",
    "hvd.broadcast_parameters(classifier.model.state_dict(), root_rank=0)\n",
    "\n",
    "#remove later\n",
    "if(hvd.rank() == 0):\n",
    "    print(\"===================== rank rank =======================\", hvd.rank())\n",
    "else:\n",
    "    print(\"===== not master rank =================================\")\n",
    "    \n",
    "\n",
    "classifier.fit(train_loader, bert_optimizer, args.epochs, NUM_GPUS, hvd.rank())\n",
    "\n",
    "#evaluation\n",
    "if(hvd.rank() == 0):\n",
    "    NUM_GPUS = 0\n",
    "    kwargs = {}\n",
    "    test_dataset = XnliDataset(file_split=TEST_FILE_SPLIT,\n",
    "                           cache_dir=CACHE_DIR,\n",
    "                           language=LANGUAGE_ENGLISH,\n",
    "                           to_lowercase=TO_LOWERCASE,\n",
    "                           tok_language=PRETRAINED_BERT_LNG\n",
    "                          )    \n",
    "    #test_sampler = SequentialSampler(test_dataset)  \n",
    "    test_loader = DataLoader(test_dataset, **kwargs)\n",
    "    start_time = time.time()\n",
    "    predictions = classifier.predict(test_loader, NUM_GPUS, BATCH_SIZE, probabilities=False)\n",
    "    end_time = time.time()\n",
    "    print(\"================= Time to predict ===========================\")\n",
    "    print(end_time - start_time)\n",
    "    print('=================== Predictions =====================')\n",
    "    print(predictions)\n",
    "\n",
    "#     test_dict = next(iter(test_loader))\n",
    "#     test_labels = test_dict['labels']\n",
    "    \n",
    "    test_labels = []\n",
    "    for data in test_dataset:\n",
    "        test_labels.append(data['labels'])\n",
    "    \n",
    "    print(len(test_labels))\n",
    "    \n",
    "    predictions= label_encoder.inverse_transform(predictions)\n",
    "    print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_COUNT = 2\n",
    "mpiConfig=MpiConfiguration()\n",
    "mpiConfig.process_count_per_node=4\n",
    "\n",
    "est = PyTorch(\n",
    "    source_directory=project_dir,\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"train.py\",\n",
    "    node_count=NODE_COUNT,\n",
    "    distributed_training=mpiConfig,\n",
    "    use_gpu=True,\n",
    "    framework_version=\"1.0\",\n",
    "    conda_packages=[\"scikit-learn=0.20.3\", \"numpy\", \"spacy\", \"nltk\"],\n",
    "    pip_packages=[\"pandas\",\"seqeval[gpu]\", \"pytorch-pretrained-bert\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version: 1.0.48\n"
     ]
    }
   ],
   "source": [
    "print(\"Azure ML SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, name=\"Nlp-Entailment-BERT\")\n",
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5673cc0f9d1c41d0a396e5dafac5f7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='outputs', model_path='outputs')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement downloads the model to local and you can use this to run predictions locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.download(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
