{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel BERT Experiments with AzureML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentEval is a widely used benchmarking tool for evaluating general-purpose sentence embeddings. It provides a simple interface for evaluating your embeddings on up to 17 supported downstream tasks (such as sentiment classification, natural language inference, semantic similarity, etc.) \n",
    "\n",
    "In this notebook, we show how to evaluate BERT sentence encodings on SentEval **in parallel** with AzureML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00 Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace, get_or_create_amlcompute\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.models.bert.sequence_encoding import BERTSentenceEncoder, PoolingStrategy\n",
    "from utils_nlp.eval.senteval import SentEvalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "NUM_GPUS = 1\n",
    "\n",
    "# # model config\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# path config\n",
    "CACHE_DIR = \"./temp\"\n",
    "PATH_TO_SENTEVAL = \"../../../SentEval\"\n",
    "\n",
    "# experiment config\n",
    "BATCH_SIZE = 32\n",
    "TRANSFER_TASKS = [\"STSBenchmark\"]\n",
    "EXP_PARAMS = {\n",
    "    \"layer_index\": [-1, -2],\n",
    "    \"pooling_strategy\": [PoolingStrategy.MEAN, PoolingStrategy.MAX],\n",
    "}\n",
    "\n",
    "# azureml config\n",
    "CONFIG_PATH = \".azureml\"\n",
    "EXPERIMENT_NAME = \"NLP-SS-bert\"\n",
    "CLUSTER_NAME = \"eval-gpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Set up AzureML resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = get_or_create_workspace(config_path=CONFIG_PATH)\n",
    "exp = Experiment(workspace=ws, name=EXPERIMENT_NAME)\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "compute = get_or_create_amlcompute(\n",
    "    workspace=ws,\n",
    "    compute_name=CLUSTER_NAME,\n",
    "    vm_size=\"STANDARD_NC6\",\n",
    "    max_nodes=8,\n",
    "    idle_seconds_before_scaledown=300,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 200 files\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/LICENSE\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/README.md\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/setup.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.gitignore\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/sts.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/binary.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__init__.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/engine.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/snli.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/utils.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/probing.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/sick.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/trec.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/mrpc.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/sst.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/rank.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/classifier.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/__init__.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/relatedness.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/ranking.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/validation.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/__pycache__/ranking.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/__pycache__/relatedness.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/__pycache__/classifier.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/__pycache__/validation.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/tools/__pycache__/__init__.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/engine.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/sick.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/sts.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/trec.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/sst.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/binary.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/rank.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/mrpc.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/snli.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/probing.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/utils.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/senteval/__pycache__/__init__.cpython-36.pyc\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/examples/skipthought.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/examples/infersent.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/examples/googleuse.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/examples/bow.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/examples/gensen.py\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/config\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/HEAD\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/description\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/index\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/packed-refs\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/objects/pack/pack-e93e3d28b4e100cfc9eba144650c4fe4922f709d.pack\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/objects/pack/pack-e93e3d28b4e100cfc9eba144650c4fe4922f709d.idx\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/info/exclude\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/logs/HEAD\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/logs/refs/heads/master\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/logs/refs/remotes/origin/HEAD\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/commit-msg.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/pre-rebase.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/pre-commit.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/applypatch-msg.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/fsmonitor-watchman.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/pre-receive.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/prepare-commit-msg.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/post-update.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/pre-applypatch.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/pre-push.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/hooks/update.sample\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/refs/heads/master\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/.git/refs/remotes/origin/HEAD\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/get_transfer_data.bash\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/tokenizer.sed\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SICK/SICK_trial.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SICK/SICK_train.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SICK/SICK_test_annotated.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/TREC/TREC_10.label-e\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/TREC/train_5500.label\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/TREC/.!49859!train_5500.label\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/TREC/TREC_10.label\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/TREC/.!49858!train_5500.label\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/MR/rt-polarity.neg\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/MR/rt-polarity.pos\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/s2.test\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/labels.dev\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/labels.train\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/s2.train\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/labels.test\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/s1.dev\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/s2.dev\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/s1.train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SNLI/s1.test\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/CR/custrev.neg\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/CR/custrev.pos\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/MRPC/msr_paraphrase_test.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/MRPC/msr_paraphrase_train.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/COCO/train.pkl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/COCO/test.pkl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/COCO/valid.pkl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/MPQA/mpqa.neg\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/MPQA/mpqa.pos\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SST/binary/sentiment-dev\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SST/binary/sentiment-test\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SST/binary/sentiment-train\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SST/fine/sentiment-dev\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SST/fine/sentiment-test\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SST/fine/sentiment-train\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SUBJ/subj.objective\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/SUBJ/subj.subjective\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.gs.ALL.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.gs.surprise.SMTnews.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.gs.SMTeuroparl.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.gs.MSRvid.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.input.MSRvid.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.gs.surprise.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.input.surprise.SMTnews.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.input.SMTeuroparl.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/00-readme.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.input.MSRpar.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.input.surprise.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS12-en-test/STS.gs.MSRpar.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.output.SMT.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.gs.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.output.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/correlation-all.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.gs.SMT.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.input.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.output.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.input.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.input.FNWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.gs.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/correct-output.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/00-readme.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/correlation.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.output.FNWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS13-en-test/STS.gs.FNWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS2016.input.plagiarism.ascii\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.input.answer-answer.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.gs.answer-answer.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.gs.postediting.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.input.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.gs.plagiarism.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.gs.question-question.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.input.plagiarism.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.input.question-question.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.gs.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS2016.input.answer-answer.ascii\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS.input.postediting.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/README.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS2016.input.headlines.ascii\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/LICENSE.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/correlation-noconfidence.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS2016.input.question-question.ascii\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS16-en-test/STS2016.input.postediting.ascii\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.gs.answers-students.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.input.answers-forums.LICENSE\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.input.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.input.answers-students.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.gs.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.input.belief.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.answers-forums.zip\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.input.answers-forums.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.gs.belief.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/corebaseline-tokencos.tar.gz\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/00-readme.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.input.images.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/correlation-noconfidence.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.gs.answers-forums.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS15-en-test/STS.gs.images.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STSBenchmark/sts-test.csv\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STSBenchmark/sts-dev.csv\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STSBenchmark/readme.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STSBenchmark/correlation.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STSBenchmark/LICENSE.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STSBenchmark/sts-train.csv\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.gs.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.input.deft-news.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.input.deft-forum.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.input.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.output.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.gs.tweet-news.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.input.OnWN.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.input.tweet-news.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.gs.headlines.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.gs.deft-forum.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/00-readme.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.input.images.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.gs.deft-news.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/sts2013-test.tgz\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/sts2012-test.tgz\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/correlation-noconfidence.pl\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/sts2012-train.tgz\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/downstream/STS/STS14-en-test/STS.gs.images.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/subj_number.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/word_content.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/past_present.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/bigram_shift.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/coordination_inversion.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/top_constituents.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/README.md\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/obj_number.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/sentence_length.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/odd_man_out.txt\n",
      "Target already exists. Skipping upload for NLP-SS-bert/senteval/data/probing/tree_depth.txt\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_e9cdbb86f83c464fb686e73f4e80f436"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload(\n",
    "    src_dir=PATH_TO_SENTEVAL,\n",
    "    target_path=os.path.join(EXPERIMENT_NAME, \"senteval\"),\n",
    "    overwrite=False,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = BERTSentenceEncoder(\n",
    "    language=LANGUAGE,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    to_lower=TO_LOWER,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Define SentEval configurations\n",
    "As specified in the SentEval repo, we implement 2 functions:\n",
    "\n",
    "**prepare** (sees the whole dataset of each task and can thus construct the word vocabulary, the dictionary of word vectors etc)\n",
    "\n",
    "**batcher** (transforms a batch of text sentences into sentence embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    sentences = [\" \".join(s).lower() for s in samples]\n",
    "    params[\"embeddings\"] = params[\"model\"].encode(\n",
    "        sentences, batch_size=params[\"batch_size\"], as_numpy=False\n",
    "    )\n",
    "    params[\"sentence2idx\"] = collections.OrderedDict(\n",
    "        list(zip(sentences, range(len(sentences))))\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [\" \".join(s).lower() for s in batch]\n",
    "    sentence_indices = [params[\"sentence2idx\"][s] for s in sentences]\n",
    "\n",
    "    df = params[\"embeddings\"]\n",
    "    embeddings = []\n",
    "    for i in sentence_indices:\n",
    "        values = np.squeeze(\n",
    "            df.loc[\n",
    "                (df[\"text_index\"] == i) & (df[\"layer_index\"] == params[\"layer_index\"])\n",
    "            ][\"values\"].values\n",
    "        ).tolist()\n",
    "        embeddings.append(values)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SentEvalConfig(\n",
    "    path=ds.path(\"{}/senteval\".format(EXPERIMENT_NAME)).as_mount(),\n",
    "    model=se,\n",
    "    prepare_func=prepare,\n",
    "    batcher_func=batcher,\n",
    "    transfer_tasks=TRANSFER_TASKS,\n",
    "    params={\"usepytorch\": True, \"batch_size\": BATCH_SIZE},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Define the script run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./temp/NLP-SS-bert/utils_nlp'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dir = os.path.join(CACHE_DIR, EXPERIMENT_NAME)\n",
    "os.makedirs(src_dir, exist_ok=True)\n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(src_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./temp/NLP-SS-bert/run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $src_dir/run.py\n",
    "import pickle\n",
    "import argparse\n",
    "from utils_nlp.eval.senteval import SentEvalConfig\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        dest=\"config\",\n",
    "        help=\"Filename of serialized SentEvalConfig object\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        dest=\"output\",\n",
    "        help=\"Filename to write serialized results to\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config = pickle.load(open(args.config, \"rb\"))\n",
    "    sys.path.insert(0, config.path)\n",
    "    import senteval\n",
    "\n",
    "    se = senteval.engine.SE(config.params, config.prepare, config.batcher)\n",
    "\n",
    "    results = se.eval(config.transfer_tasks)\n",
    "    pickle.dump(results, open(args.output, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 Run experiments in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_groups = list(itertools.product(*list(EXP_PARAMS.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "    ],\n",
    "    pip_packages=[\"azureml-sdk==1.0.43.*\", \n",
    "                  \"torch==1.1\", \n",
    "                  \"tqdm==4.31.1\",\n",
    "                 \"pytorch-pretrained-bert>=0.6\"],\n",
    "    python_version=\"3.6.8\",\n",
    ")\n",
    "\n",
    "rc = RunConfiguration(conda_dependencies=conda_dependencies)\n",
    "rc.target = CLUSTER_NAME\n",
    "rc.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "for i, p in enumerate(parameter_groups):\n",
    "    exp_params = dict(zip(EXP_PARAMS.keys(), p))\n",
    "    sc = deepcopy(sec)\n",
    "    sc.append_params(exp_params)\n",
    "    for k, v in exp_params.items():\n",
    "        setattr(sc.model, k, v)\n",
    "\n",
    "    pickle.dump(sc, open(os.path.join(CACHE_DIR, \"config{0:03d}.pkl\".format(i)), \"wb\"))\n",
    "    ds.upload_files(\n",
    "        [os.path.join(CACHE_DIR, \"config{0:03d}.pkl\".format(i))],\n",
    "        target_path=EXPERIMENT_NAME,\n",
    "        overwrite=False,\n",
    "        show_progress=False,\n",
    "    )\n",
    "\n",
    "    input_config = DataReference(\n",
    "        datastore=ds,\n",
    "        data_reference_name=\"config{0:03d}\".format(i),\n",
    "        path_on_datastore=\"{0}/config{1:03d}.pkl\".format(EXPERIMENT_NAME, i),\n",
    "    )\n",
    "    output_results = PipelineData(\n",
    "        datastore=ds,\n",
    "        name=\"results{0:03d}\".format(i),\n",
    "        output_path_on_compute=\"{0}/results{1:03d}.pkl\".format(EXPERIMENT_NAME, i),\n",
    "    )\n",
    "\n",
    "    step = PythonScriptStep(\n",
    "        source_directory=src_dir,\n",
    "        script_name=\"run.py\",\n",
    "        arguments=[\n",
    "            \"--config\",\n",
    "            input_config,\n",
    "            \"--output\",\n",
    "            output_results,\n",
    "        ],\n",
    "        inputs=[input_config],\n",
    "        outputs=[output_results],\n",
    "        runconfig=rc,\n",
    "    )\n",
    "\n",
    "    steps.append(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step run.py is ready to be created [d0fba441]\n",
      "Step run.py is ready to be created [a90bd4e1]\n",
      "Step run.py is ready to be created [34ebeb9e]\n",
      "Step run.py is ready to be created [481b4441]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step run.py [d0fba441][c68d387a-e3b1-4f4e-b602-7d44645f478c], (This step will run and generate new outputs)\n",
      "Created step run.py [a90bd4e1][d0e78112-92a5-41d8-976a-f92c038d909d], (This step will run and generate new outputs)\n",
      "Created step run.py [34ebeb9e][68b9c214-29d1-4ffb-a0f0-47616f676d35], (This step will run and generate new outputs)\n",
      "Created step run.py [481b4441][ef9c270e-5884-443f-85b0-cecbb5c37eac], (This step will run and generate new outputs)\n",
      "Using data reference config000 for StepId [ad497262][3fb28726-32ae-48a5-b91d-f23228d42611], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference config001 for StepId [af28eb33][95ed72e0-6882-4bc2-803f-727c945ad703], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference config002 for StepId [0554c17c][c0fc39fd-63f1-41c1-af53-9fb56ccb5c2f], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference config003 for StepId [d62120c0][02844cfe-8247-4705-8b3c-9f8ce0a167c4], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: f73f17e7-5e76-4f17-b165-e2eebabaf00b\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = exp.submit(pipeline, regenerate_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36965d941e84be5a8fd31999a0487c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_cpu)",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
