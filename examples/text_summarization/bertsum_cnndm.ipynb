{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summerization on CNN/DM Dataset using BertSum\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune BERT for extractive text summerization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization (https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/). Extractive summarization are usually used in document summarization where each input document consists of mutiple sentences. The preprocessing of the input training data involves assigning label 0 or 1 to the document sentences based on the give summary. The summarization problem is also simplfied to classifying whether each document sentence should be included in the summary. \n",
    "\n",
    "The figure below illustrates how BERTSum can be fine tuned for extractive summarization task. Each sentence is inserted with [CLS] token at the beginning and  [SEP] at the end. Interval segment embedding and positional embedding are added upon the token embedding before input the BERT model. The [CLS] token representation is used as sentence embedding and only the [CLS] tokens are used as input for the summarization model. The summarization layer predicts whether the probability of each each sentence token should be included in the summary or not. Techniques like trigram blocking can be used to improve model accuarcy.   \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertSum.PNG\">\n",
    "\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|USE_PREPROCESSED_DATA|encoder|Machine Configurations|Running time|\n",
    "|:---------|:---------|:---------|:----------------------|:------------|\n",
    "|True|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|False|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 60 minutes |\n",
    "|True|False|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|True|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 6.5hours |\n",
    "|True|False|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|False|any| 1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| > 24 hours|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True\n",
    "USE_PREPROCESSED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to clone a modified version of BertSum so that it works for prediction cases and can run on any GPU device ID on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-15 17:28:02--  https://raw.githubusercontent.com/nlpyang/BertSum/master/bert_config_uncased_base.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.124.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.124.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 313 [text/plain]\n",
      "Saving to: ‘bert_config_uncased_base.json’\n",
      "\n",
      "bert_config_uncased 100%[===================>]     313  --.-KB/s    in 0s      \n",
      "\n",
      "2019-10-15 17:28:02 (59.7 MB/s) - ‘bert_config_uncased_base.json’ saved [313/313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/nlpyang/BertSum/master/bert_config_uncased_base.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CONFIG_PATH=\"./bert_config_uncased_base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "sys.path.insert(0, \"./\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to install the dependencies for pyrouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://azure.archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Ign:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:4 http://azure.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Hit:6 https://packages.microsoft.com/repos/microsoft-ubuntu-xenial-prod xenial InRelease\n",
      "Hit:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Fetched 252 kB in 1s (350 kB/s)                    \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "expat is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-headers-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1018\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'libexpat1-dev' instead of 'libexpat-dev'\n",
      "libexpat1-dev is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-headers-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1018\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# dependencies for ROUGE-1.5.5.pl\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install expat\n",
    "!sudo apt-get install libexpat-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal to install pre-requiste for using pyrouge.\n",
    "1. sudo cpan install XML::Parser\n",
    "1. sudo cpan install XML::Parser::PerlSAX\n",
    "1. sudo cpan install XML::DOM\n",
    "\n",
    "Download ROUGE-1.5.5 from https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5.\n",
    "Run the following command in your terminal.\n",
    "* pyrouge_set_rouge_path $ABSOLUTE_DIRECTORY_TO_ROUGE-1.5.5.pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation and ~11K test dataset.  You can choose to use the preprocessed version at [BERTSum published example](https://github.com/nlpyang/BertSum/) or use the following section to preprocess the data. Since it takes up to 28 hours to preprocess the training data  to run on 10  Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz, we suggest you continue with set as True first and experiment with data preprocessing  with QUICKRUN set as True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Preprocessed Data\n",
    "Please go to  [BERTSum published example](https://github.com/nlpyang/BertSum/) to download preprocessed data and unzip it to the folder \"./bert_data\" at the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREPROCESSED_DATA =  True\n",
    "if USE_PREPROCESSED_DATA:\n",
    "    BERT_DATA_PATH=\"./bert_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to use preprocessed data, continue to section Model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Data Preprocessing\n",
    "To continue with the data preprocessing, run the following command to download from https://github.com/harvardnlp/sent-summary and unzip the data to folder ./harvardnlp_cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-15 17:28:19--  https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.238.221\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.238.221|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 500375629 (477M) [application/x-gzip]\n",
      "Saving to: ‘cnndm.tar.gz’\n",
      "\n",
      "cnndm.tar.gz        100%[===================>] 477.20M  91.7MB/s    in 5.2s    \n",
      "\n",
      "2019-10-15 17:28:24 (91.4 MB/s) - ‘cnndm.tar.gz’ saved [500375629/500375629]\n",
      "\n",
      "test.txt.src\n",
      "test.txt.tgt.tagged\n",
      "train.txt.src\n",
      "train.txt.tgt.tagged\n",
      "val.txt.src\n",
      "val.txt.tgt.tagged\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz &&\\\n",
    "    mkdir -p harvardnlp_cnndm &&\\\n",
    "    mv cnndm.tar.gz ./harvardnlp_cnndm && cd ./harvardnlp_cnndm &&\\\n",
    "    tar -xvf cnndm.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details of Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to process the input articles to the format that BertSum takes.  Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line-breaker seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. label the sentences in the article with 1 meaning the sentence is selected and 0 meaning the sentence is not selected. The options for the selection algorithms are \"greedy\" and \"combination\"\n",
    "3. convert each example to  BertSum format\n",
    "    - filter the sentences in the example based on the min_src_ntokens argument. If the lefted total sentence number is less than min_nsents, the example is discarded.\n",
    "    - truncate the sentences in the example if the length is greater than max_src_ntokens\n",
    "    - truncate the sentences in the example and the labels if the totle number of sentences is greater than max_nsents\n",
    "    - [CLS] and [SEP] are inserted before and after each sentence\n",
    "    - wordPiece tokenization\n",
    "    - truncate the example to 512 tokens\n",
    "    - convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "    - segment ids are generated\n",
    "    - [CLS] token positions are logged\n",
    "    - [CLS] token labels are truncated if it's greater than 512, which is the maximum input length that can be taken by the BERT model.\n",
    "    \n",
    "    \n",
    "Note that the original BERTSum paper use Stanford CoreNLP for data proprocessing, here we'll first how to use NLTK version, and then we also provide instruction of how to set up Stanford NLP and code examples of how to use Standford CoreNLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.dataset.harvardnlp_cnndm import harvardnlp_cnndm_preprocess\n",
    "from utils_nlp.models.bert.extractive_text_summarization import bertsum_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_train_job_number = -1\n",
    "max_test_job_number = -1\n",
    "if QUICK_RUN:\n",
    "    max_train_job_number = 100\n",
    "    max_test_job_number = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 100\n",
      "CPU times: user 2.98 s, sys: 2.49 s, total: 5.46 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_SRC_FILE = \"./harvardnlp_cnndm/train.txt.src\"\n",
    "TRAIN_TGT_FILE = \"./harvardnlp_cnndm/train.txt.tgt.tagged\"\n",
    "PROCESSED_TRAIN_FILE = f\"./harvardnlp_cnndm/train.bertdata_{QUICK_RUN}\" \n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TRAIN_SRC_FILE, TRAIN_TGT_FILE, max_train_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from bertsum.prepro.data_builder import BertData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_train_job_number], PROCESSED_TRAIN_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 100\n",
      "CPU times: user 586 ms, sys: 1.01 s, total: 1.59 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_SRC_FILE = \"./harvardnlp_cnndm/test.txt.src\"\n",
    "TEST_TGT_FILE = \"./harvardnlp_cnndm/test.txt.tgt.tagged\"\n",
    "PROCESSED_TEST_FILE = f\"./harvardnlp_cnndm/test.bertdata_{QUICK_RUN}\" \n",
    "\n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TEST_SRC_FILE, TEST_TGT_FILE, max_test_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from bertsum.prepro.data_builder import BertData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_test_job_number], PROCESSED_TEST_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': [['marseille',\n",
       "   ',',\n",
       "   'france',\n",
       "   '(',\n",
       "   'cnn',\n",
       "   ')',\n",
       "   'the',\n",
       "   'french',\n",
       "   'prosecutor',\n",
       "   'leading',\n",
       "   'an',\n",
       "   'investigation',\n",
       "   'into',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'of',\n",
       "   'germanwings',\n",
       "   'flight',\n",
       "   '9525',\n",
       "   'insisted',\n",
       "   'wednesday',\n",
       "   'that',\n",
       "   'he',\n",
       "   'was',\n",
       "   'not',\n",
       "   'aware',\n",
       "   'of',\n",
       "   'any',\n",
       "   'video',\n",
       "   'footage',\n",
       "   'from',\n",
       "   'on',\n",
       "   'board',\n",
       "   'the',\n",
       "   'plane',\n",
       "   '.'],\n",
       "  ['marseille',\n",
       "   'prosecutor',\n",
       "   'brice',\n",
       "   'robin',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'that',\n",
       "   '``',\n",
       "   'so',\n",
       "   'far',\n",
       "   'no',\n",
       "   'videos',\n",
       "   'were',\n",
       "   'used',\n",
       "   'in',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'investigation',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['he',\n",
       "   'added',\n",
       "   ',',\n",
       "   '``',\n",
       "   'a',\n",
       "   'person',\n",
       "   'who',\n",
       "   'has',\n",
       "   'such',\n",
       "   'a',\n",
       "   'video',\n",
       "   'needs',\n",
       "   'to',\n",
       "   'immediately',\n",
       "   'give',\n",
       "   'it',\n",
       "   'to',\n",
       "   'the',\n",
       "   'investigators',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['robin',\n",
       "   \"'s\",\n",
       "   'comments',\n",
       "   'follow',\n",
       "   'claims',\n",
       "   'by',\n",
       "   'two',\n",
       "   'magazines',\n",
       "   ',',\n",
       "   'german',\n",
       "   'daily',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'french',\n",
       "   'paris',\n",
       "   'match',\n",
       "   ',',\n",
       "   'of',\n",
       "   'a',\n",
       "   'cell',\n",
       "   'phone',\n",
       "   'video',\n",
       "   'showing',\n",
       "   'the',\n",
       "   'harrowing',\n",
       "   'final',\n",
       "   'seconds',\n",
       "   'from',\n",
       "   'on',\n",
       "   'board',\n",
       "   'germanwings',\n",
       "   'flight',\n",
       "   '9525',\n",
       "   'as',\n",
       "   'it',\n",
       "   'crashed',\n",
       "   'into',\n",
       "   'the',\n",
       "   'french',\n",
       "   'alps',\n",
       "   '.'],\n",
       "  ['all', '150', 'on', 'board', 'were', 'killed', '.'],\n",
       "  ['paris',\n",
       "   'match',\n",
       "   'and',\n",
       "   'bild',\n",
       "   'reported',\n",
       "   'that',\n",
       "   'the',\n",
       "   'video',\n",
       "   'was',\n",
       "   'recovered',\n",
       "   'from',\n",
       "   'a',\n",
       "   'phone',\n",
       "   'at',\n",
       "   'the',\n",
       "   'wreckage',\n",
       "   'site',\n",
       "   '.'],\n",
       "  ['the',\n",
       "   'two',\n",
       "   'publications',\n",
       "   'described',\n",
       "   'the',\n",
       "   'supposed',\n",
       "   'video',\n",
       "   ',',\n",
       "   'but',\n",
       "   'did',\n",
       "   'not',\n",
       "   'post',\n",
       "   'it',\n",
       "   'on',\n",
       "   'their',\n",
       "   'websites',\n",
       "   '.'],\n",
       "  ['the',\n",
       "   'publications',\n",
       "   'said',\n",
       "   'that',\n",
       "   'they',\n",
       "   'watched',\n",
       "   'the',\n",
       "   'video',\n",
       "   ',',\n",
       "   'which',\n",
       "   'was',\n",
       "   'found',\n",
       "   'by',\n",
       "   'a',\n",
       "   'source',\n",
       "   'close',\n",
       "   'to',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['one',\n",
       "   'can',\n",
       "   'hear',\n",
       "   'cries',\n",
       "   'of',\n",
       "   '`',\n",
       "   'my',\n",
       "   'god',\n",
       "   \"'\",\n",
       "   'in',\n",
       "   'several',\n",
       "   'languages',\n",
       "   ',',\n",
       "   '``',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'reported',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['metallic',\n",
       "   'banging',\n",
       "   'can',\n",
       "   'also',\n",
       "   'be',\n",
       "   'heard',\n",
       "   'more',\n",
       "   'than',\n",
       "   'three',\n",
       "   'times',\n",
       "   ',',\n",
       "   'perhaps',\n",
       "   'of',\n",
       "   'the',\n",
       "   'pilot',\n",
       "   'trying',\n",
       "   'to',\n",
       "   'open',\n",
       "   'the',\n",
       "   'cockpit',\n",
       "   'door',\n",
       "   'with',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'object',\n",
       "   '.'],\n",
       "  ['towards',\n",
       "   'the',\n",
       "   'end',\n",
       "   ',',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'shake',\n",
       "   ',',\n",
       "   'stronger',\n",
       "   'than',\n",
       "   'the',\n",
       "   'others',\n",
       "   ',',\n",
       "   'the',\n",
       "   'screaming',\n",
       "   'intensifies',\n",
       "   '.'],\n",
       "  ['then', 'nothing', '.', '``'],\n",
       "  ['``',\n",
       "   'it',\n",
       "   'is',\n",
       "   'a',\n",
       "   'very',\n",
       "   'disturbing',\n",
       "   'scene',\n",
       "   ',',\n",
       "   '``',\n",
       "   'said',\n",
       "   'julian',\n",
       "   'reichelt',\n",
       "   ',',\n",
       "   'editor-in-chief',\n",
       "   'of',\n",
       "   'bild',\n",
       "   'online',\n",
       "   '.'],\n",
       "  ['an',\n",
       "   'official',\n",
       "   'with',\n",
       "   'france',\n",
       "   \"'s\",\n",
       "   'accident',\n",
       "   'investigation',\n",
       "   'agency',\n",
       "   ',',\n",
       "   'the',\n",
       "   'bea',\n",
       "   ',',\n",
       "   'said',\n",
       "   'the',\n",
       "   'agency',\n",
       "   'is',\n",
       "   'not',\n",
       "   'aware',\n",
       "   'of',\n",
       "   'any',\n",
       "   'such',\n",
       "   'video',\n",
       "   '.'],\n",
       "  ['lt.',\n",
       "   'col.',\n",
       "   'jean-marc',\n",
       "   'menichini',\n",
       "   ',',\n",
       "   'a',\n",
       "   'french',\n",
       "   'gendarmerie',\n",
       "   'spokesman',\n",
       "   'in',\n",
       "   'charge',\n",
       "   'of',\n",
       "   'communications',\n",
       "   'on',\n",
       "   'rescue',\n",
       "   'efforts',\n",
       "   'around',\n",
       "   'the',\n",
       "   'germanwings',\n",
       "   'crash',\n",
       "   'site',\n",
       "   ',',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'that',\n",
       "   'the',\n",
       "   'reports',\n",
       "   'were',\n",
       "   '``',\n",
       "   'completely',\n",
       "   'wrong',\n",
       "   '``',\n",
       "   'and',\n",
       "   '``',\n",
       "   'unwarranted',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['cell',\n",
       "   'phones',\n",
       "   'have',\n",
       "   'been',\n",
       "   'collected',\n",
       "   'at',\n",
       "   'the',\n",
       "   'site',\n",
       "   ',',\n",
       "   'he',\n",
       "   'said',\n",
       "   ',',\n",
       "   'but',\n",
       "   'that',\n",
       "   'they',\n",
       "   '``',\n",
       "   'had',\n",
       "   \"n't\",\n",
       "   'been',\n",
       "   'exploited',\n",
       "   'yet',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['menichini',\n",
       "   'said',\n",
       "   'he',\n",
       "   'believed',\n",
       "   'the',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'would',\n",
       "   'need',\n",
       "   'to',\n",
       "   'be',\n",
       "   'sent',\n",
       "   'to',\n",
       "   'the',\n",
       "   'criminal',\n",
       "   'research',\n",
       "   'institute',\n",
       "   'in',\n",
       "   'rosny',\n",
       "   'sous-bois',\n",
       "   ',',\n",
       "   'near',\n",
       "   'paris',\n",
       "   ',',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'be',\n",
       "   'analyzed',\n",
       "   'by',\n",
       "   'specialized',\n",
       "   'technicians',\n",
       "   'working',\n",
       "   'hand-in-hand',\n",
       "   'with',\n",
       "   'investigators',\n",
       "   '.'],\n",
       "  ['but',\n",
       "   'none',\n",
       "   'of',\n",
       "   'the',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'found',\n",
       "   'so',\n",
       "   'far',\n",
       "   'have',\n",
       "   'been',\n",
       "   'sent',\n",
       "   'to',\n",
       "   'the',\n",
       "   'institute',\n",
       "   ',',\n",
       "   'menichini',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['asked',\n",
       "   'whether',\n",
       "   'staff',\n",
       "   'involved',\n",
       "   'in',\n",
       "   'the',\n",
       "   'search',\n",
       "   'could',\n",
       "   'have',\n",
       "   'leaked',\n",
       "   'a',\n",
       "   'memory',\n",
       "   'card',\n",
       "   'to',\n",
       "   'the',\n",
       "   'media',\n",
       "   ',',\n",
       "   'menichini',\n",
       "   'answered',\n",
       "   'with',\n",
       "   'a',\n",
       "   'categorical',\n",
       "   '``',\n",
       "   'no',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['reichelt',\n",
       "   'told',\n",
       "   '``',\n",
       "   'erin',\n",
       "   'burnett',\n",
       "   ':',\n",
       "   'outfront',\n",
       "   '``',\n",
       "   'that',\n",
       "   'he',\n",
       "   'had',\n",
       "   'watched',\n",
       "   'the',\n",
       "   'video',\n",
       "   'and',\n",
       "   'stood',\n",
       "   'by',\n",
       "   'the',\n",
       "   'report',\n",
       "   ',',\n",
       "   'saying',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'are',\n",
       "   '``',\n",
       "   'very',\n",
       "   'confident',\n",
       "   '``',\n",
       "   'that',\n",
       "   'the',\n",
       "   'clip',\n",
       "   'is',\n",
       "   'real',\n",
       "   '.'],\n",
       "  ['he',\n",
       "   'noted',\n",
       "   'that',\n",
       "   'investigators',\n",
       "   'only',\n",
       "   'revealed',\n",
       "   'they',\n",
       "   \"'d\",\n",
       "   'recovered',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'from',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'site',\n",
       "   'after',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'published',\n",
       "   'their',\n",
       "   'reports',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['that', 'is', 'something', 'we', 'did', 'not', 'know', 'before', '.'],\n",
       "  ['...',\n",
       "   'overall',\n",
       "   'we',\n",
       "   'can',\n",
       "   'say',\n",
       "   'many',\n",
       "   'things',\n",
       "   'of',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'were',\n",
       "   \"n't\",\n",
       "   'revealed',\n",
       "   'by',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'at',\n",
       "   'the',\n",
       "   'beginning',\n",
       "   ',',\n",
       "   '``',\n",
       "   'he',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['what', 'was', 'mental', 'state', 'of', 'germanwings', 'co-pilot', '?'],\n",
       "  ['german',\n",
       "   'airline',\n",
       "   'lufthansa',\n",
       "   'confirmed',\n",
       "   'tuesday',\n",
       "   'that',\n",
       "   'co-pilot',\n",
       "   'andreas',\n",
       "   'lubitz',\n",
       "   'had',\n",
       "   'battled',\n",
       "   'depression',\n",
       "   'years',\n",
       "   'before',\n",
       "   'he',\n",
       "   'took',\n",
       "   'the',\n",
       "   'controls',\n",
       "   'of',\n",
       "   'germanwings',\n",
       "   'flight',\n",
       "   '9525',\n",
       "   ',',\n",
       "   'which',\n",
       "   'he',\n",
       "   \"'s\",\n",
       "   'accused',\n",
       "   'of',\n",
       "   'deliberately',\n",
       "   'crashing',\n",
       "   'last',\n",
       "   'week',\n",
       "   'in',\n",
       "   'the',\n",
       "   'french',\n",
       "   'alps',\n",
       "   '.'],\n",
       "  ['lubitz',\n",
       "   'told',\n",
       "   'his',\n",
       "   'lufthansa',\n",
       "   'flight',\n",
       "   'training',\n",
       "   'school',\n",
       "   'in',\n",
       "   '2009',\n",
       "   'that',\n",
       "   'he',\n",
       "   'had',\n",
       "   'a',\n",
       "   '``',\n",
       "   'previous',\n",
       "   'episode',\n",
       "   'of',\n",
       "   'severe',\n",
       "   'depression',\n",
       "   ',',\n",
       "   '``',\n",
       "   'the',\n",
       "   'airline',\n",
       "   'said',\n",
       "   'tuesday',\n",
       "   '.'],\n",
       "  ['email',\n",
       "   'correspondence',\n",
       "   'between',\n",
       "   'lubitz',\n",
       "   'and',\n",
       "   'the',\n",
       "   'school',\n",
       "   'discovered',\n",
       "   'in',\n",
       "   'an',\n",
       "   'internal',\n",
       "   'investigation',\n",
       "   ',',\n",
       "   'lufthansa',\n",
       "   'said',\n",
       "   ',',\n",
       "   'included',\n",
       "   'medical',\n",
       "   'documents',\n",
       "   'he',\n",
       "   'submitted',\n",
       "   'in',\n",
       "   'connection',\n",
       "   'with',\n",
       "   'resuming',\n",
       "   'his',\n",
       "   'flight',\n",
       "   'training',\n",
       "   '.'],\n",
       "  ['the',\n",
       "   'announcement',\n",
       "   'indicates',\n",
       "   'that',\n",
       "   'lufthansa',\n",
       "   ',',\n",
       "   'the',\n",
       "   'parent',\n",
       "   'company',\n",
       "   'of',\n",
       "   'germanwings',\n",
       "   ',',\n",
       "   'knew',\n",
       "   'of',\n",
       "   'lubitz',\n",
       "   \"'s\",\n",
       "   'battle',\n",
       "   'with',\n",
       "   'depression',\n",
       "   ',',\n",
       "   'allowed',\n",
       "   'him',\n",
       "   'to',\n",
       "   'continue',\n",
       "   'training',\n",
       "   'and',\n",
       "   'ultimately',\n",
       "   'put',\n",
       "   'him',\n",
       "   'in',\n",
       "   'the',\n",
       "   'cockpit',\n",
       "   '.'],\n",
       "  ['lufthansa',\n",
       "   ',',\n",
       "   'whose',\n",
       "   'ceo',\n",
       "   'carsten',\n",
       "   'spohr',\n",
       "   'previously',\n",
       "   'said',\n",
       "   'lubitz',\n",
       "   'was',\n",
       "   '100',\n",
       "   '%',\n",
       "   'fit',\n",
       "   'to',\n",
       "   'fly',\n",
       "   ',',\n",
       "   'described',\n",
       "   'its',\n",
       "   'statement',\n",
       "   'tuesday',\n",
       "   'as',\n",
       "   'a',\n",
       "   '``',\n",
       "   'swift',\n",
       "   'and',\n",
       "   'seamless',\n",
       "   'clarification',\n",
       "   '``',\n",
       "   'and',\n",
       "   'said',\n",
       "   'it',\n",
       "   'was',\n",
       "   'sharing',\n",
       "   'the',\n",
       "   'information',\n",
       "   'and',\n",
       "   'documents',\n",
       "   '--',\n",
       "   'including',\n",
       "   'training',\n",
       "   'and',\n",
       "   'medical',\n",
       "   'records',\n",
       "   '--',\n",
       "   'with',\n",
       "   'public',\n",
       "   'prosecutors',\n",
       "   '.'],\n",
       "  ['spohr',\n",
       "   'traveled',\n",
       "   'to',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'site',\n",
       "   'wednesday',\n",
       "   ',',\n",
       "   'where',\n",
       "   'recovery',\n",
       "   'teams',\n",
       "   'have',\n",
       "   'been',\n",
       "   'working',\n",
       "   'for',\n",
       "   'the',\n",
       "   'past',\n",
       "   'week',\n",
       "   'to',\n",
       "   'recover',\n",
       "   'human',\n",
       "   'remains',\n",
       "   'and',\n",
       "   'plane',\n",
       "   'debris',\n",
       "   'scattered',\n",
       "   'across',\n",
       "   'a',\n",
       "   'steep',\n",
       "   'mountainside',\n",
       "   '.'],\n",
       "  ['he',\n",
       "   'saw',\n",
       "   'the',\n",
       "   'crisis',\n",
       "   'center',\n",
       "   'set',\n",
       "   'up',\n",
       "   'in',\n",
       "   'seyne-les-alpes',\n",
       "   ',',\n",
       "   'laid',\n",
       "   'a',\n",
       "   'wreath',\n",
       "   'in',\n",
       "   'the',\n",
       "   'village',\n",
       "   'of',\n",
       "   'le',\n",
       "   'vernet',\n",
       "   ',',\n",
       "   'closer',\n",
       "   'to',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'site',\n",
       "   ',',\n",
       "   'where',\n",
       "   'grieving',\n",
       "   'families',\n",
       "   'have',\n",
       "   'left',\n",
       "   'flowers',\n",
       "   'at',\n",
       "   'a',\n",
       "   'simple',\n",
       "   'stone',\n",
       "   'memorial',\n",
       "   '.'],\n",
       "  ['menichini',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'late',\n",
       "   'tuesday',\n",
       "   'that',\n",
       "   'no',\n",
       "   'visible',\n",
       "   'human',\n",
       "   'remains',\n",
       "   'were',\n",
       "   'left',\n",
       "   'at',\n",
       "   'the',\n",
       "   'site',\n",
       "   'but',\n",
       "   'recovery',\n",
       "   'teams',\n",
       "   'would',\n",
       "   'keep',\n",
       "   'searching',\n",
       "   '.'],\n",
       "  ['french',\n",
       "   'president',\n",
       "   'francois',\n",
       "   'hollande',\n",
       "   ',',\n",
       "   'speaking',\n",
       "   'tuesday',\n",
       "   ',',\n",
       "   'said',\n",
       "   'that',\n",
       "   'it',\n",
       "   'should',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'all',\n",
       "   'the',\n",
       "   'victims',\n",
       "   'using',\n",
       "   'dna',\n",
       "   'analysis',\n",
       "   'by',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'the',\n",
       "   'week',\n",
       "   ',',\n",
       "   'sooner',\n",
       "   'than',\n",
       "   'authorities',\n",
       "   'had',\n",
       "   'previously',\n",
       "   'suggested',\n",
       "   '.'],\n",
       "  ['in',\n",
       "   'the',\n",
       "   'meantime',\n",
       "   ',',\n",
       "   'the',\n",
       "   'recovery',\n",
       "   'of',\n",
       "   'the',\n",
       "   'victims',\n",
       "   \"'\",\n",
       "   'personal',\n",
       "   'belongings',\n",
       "   'will',\n",
       "   'start',\n",
       "   'wednesday',\n",
       "   ',',\n",
       "   'menichini',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['among',\n",
       "   'those',\n",
       "   'personal',\n",
       "   'belongings',\n",
       "   'could',\n",
       "   'be',\n",
       "   'more',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'belonging',\n",
       "   'to',\n",
       "   'the',\n",
       "   '144',\n",
       "   'passengers',\n",
       "   'and',\n",
       "   'six',\n",
       "   'crew',\n",
       "   'on',\n",
       "   'board',\n",
       "   '.'],\n",
       "  ['check', 'out', 'the', 'latest', 'from', 'our', 'correspondents', '.'],\n",
       "  ['the',\n",
       "   'details',\n",
       "   'about',\n",
       "   'lubitz',\n",
       "   \"'s\",\n",
       "   'correspondence',\n",
       "   'with',\n",
       "   'the',\n",
       "   'flight',\n",
       "   'school',\n",
       "   'during',\n",
       "   'his',\n",
       "   'training',\n",
       "   'were',\n",
       "   'among',\n",
       "   'several',\n",
       "   'developments',\n",
       "   'as',\n",
       "   'investigators',\n",
       "   'continued',\n",
       "   'to',\n",
       "   'delve',\n",
       "   'into',\n",
       "   'what',\n",
       "   'caused',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'and',\n",
       "   'lubitz',\n",
       "   \"'s\",\n",
       "   'possible',\n",
       "   'motive',\n",
       "   'for',\n",
       "   'downing',\n",
       "   'the',\n",
       "   'jet',\n",
       "   '.'],\n",
       "  ['a',\n",
       "   'lufthansa',\n",
       "   'spokesperson',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'on',\n",
       "   'tuesday',\n",
       "   'that',\n",
       "   'lubitz',\n",
       "   'had',\n",
       "   'a',\n",
       "   'valid',\n",
       "   'medical',\n",
       "   'certificate',\n",
       "   ',',\n",
       "   'had',\n",
       "   'passed',\n",
       "   'all',\n",
       "   'his',\n",
       "   'examinations',\n",
       "   'and',\n",
       "   '``',\n",
       "   'held',\n",
       "   'all',\n",
       "   'the',\n",
       "   'licenses',\n",
       "   'required',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['earlier',\n",
       "   ',',\n",
       "   'a',\n",
       "   'spokesman',\n",
       "   'for',\n",
       "   'the',\n",
       "   'prosecutor',\n",
       "   \"'s\",\n",
       "   'office',\n",
       "   'in',\n",
       "   'dusseldorf',\n",
       "   ',',\n",
       "   'christoph',\n",
       "   'kumpa',\n",
       "   ',',\n",
       "   'said',\n",
       "   'medical',\n",
       "   'records',\n",
       "   'reveal',\n",
       "   'lubitz',\n",
       "   'suffered',\n",
       "   'from',\n",
       "   'suicidal',\n",
       "   'tendencies',\n",
       "   'at',\n",
       "   'some',\n",
       "   'point',\n",
       "   'before',\n",
       "   'his',\n",
       "   'aviation',\n",
       "   'career',\n",
       "   'and',\n",
       "   'underwent',\n",
       "   'psychotherapy',\n",
       "   'before',\n",
       "   'he',\n",
       "   'got',\n",
       "   'his',\n",
       "   'pilot',\n",
       "   \"'s\",\n",
       "   'license',\n",
       "   '.'],\n",
       "  ['kumpa',\n",
       "   'emphasized',\n",
       "   'there',\n",
       "   \"'s\",\n",
       "   'no',\n",
       "   'evidence',\n",
       "   'suggesting',\n",
       "   'lubitz',\n",
       "   'was',\n",
       "   'suicidal',\n",
       "   'or',\n",
       "   'acting',\n",
       "   'aggressively',\n",
       "   'before',\n",
       "   'the',\n",
       "   'crash',\n",
       "   '.'],\n",
       "  ['investigators',\n",
       "   'are',\n",
       "   'looking',\n",
       "   'into',\n",
       "   'whether',\n",
       "   'lubitz',\n",
       "   'feared',\n",
       "   'his',\n",
       "   'medical',\n",
       "   'condition',\n",
       "   'would',\n",
       "   'cause',\n",
       "   'him',\n",
       "   'to',\n",
       "   'lose',\n",
       "   'his',\n",
       "   'pilot',\n",
       "   \"'s\",\n",
       "   'license',\n",
       "   ',',\n",
       "   'a',\n",
       "   'european',\n",
       "   'government',\n",
       "   'official',\n",
       "   'briefed',\n",
       "   'on',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'on',\n",
       "   'tuesday',\n",
       "   '.'],\n",
       "  ['while',\n",
       "   'flying',\n",
       "   'was',\n",
       "   '``',\n",
       "   'a',\n",
       "   'big',\n",
       "   'part',\n",
       "   'of',\n",
       "   'his',\n",
       "   'life',\n",
       "   ',',\n",
       "   '``',\n",
       "   'the',\n",
       "   'source',\n",
       "   'said',\n",
       "   ',',\n",
       "   'it',\n",
       "   \"'s\",\n",
       "   'only',\n",
       "   'one',\n",
       "   'theory',\n",
       "   'being',\n",
       "   'considered',\n",
       "   '.'],\n",
       "  ['another',\n",
       "   'source',\n",
       "   ',',\n",
       "   'a',\n",
       "   'law',\n",
       "   'enforcement',\n",
       "   'official',\n",
       "   'briefed',\n",
       "   'on',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   ',',\n",
       "   'also',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'that',\n",
       "   'authorities',\n",
       "   'believe',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'motive',\n",
       "   'for',\n",
       "   'lubitz',\n",
       "   'to',\n",
       "   'bring',\n",
       "   'down',\n",
       "   'the',\n",
       "   'plane',\n",
       "   'was',\n",
       "   'that',\n",
       "   'he',\n",
       "   'feared',\n",
       "   'he',\n",
       "   'would',\n",
       "   'not',\n",
       "   'be',\n",
       "   'allowed',\n",
       "   'to',\n",
       "   'fly',\n",
       "   'because',\n",
       "   'of',\n",
       "   'his',\n",
       "   'medical',\n",
       "   'problems',\n",
       "   '.'],\n",
       "  ['lubitz',\n",
       "   \"'s\",\n",
       "   'girlfriend',\n",
       "   'told',\n",
       "   'investigators',\n",
       "   'he',\n",
       "   'had',\n",
       "   'seen',\n",
       "   'an',\n",
       "   'eye',\n",
       "   'doctor',\n",
       "   'and',\n",
       "   'a',\n",
       "   'neuropsychologist',\n",
       "   ',',\n",
       "   'both',\n",
       "   'of',\n",
       "   'whom',\n",
       "   'deemed',\n",
       "   'him',\n",
       "   'unfit',\n",
       "   'to',\n",
       "   'work',\n",
       "   'recently',\n",
       "   'and',\n",
       "   'concluded',\n",
       "   'he',\n",
       "   'had',\n",
       "   'psychological',\n",
       "   'issues',\n",
       "   ',',\n",
       "   'the',\n",
       "   'european',\n",
       "   'government',\n",
       "   'official',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['but',\n",
       "   'no',\n",
       "   'matter',\n",
       "   'what',\n",
       "   'details',\n",
       "   'emerge',\n",
       "   'about',\n",
       "   'his',\n",
       "   'previous',\n",
       "   'mental',\n",
       "   'health',\n",
       "   'struggles',\n",
       "   ',',\n",
       "   'there',\n",
       "   \"'s\",\n",
       "   'more',\n",
       "   'to',\n",
       "   'the',\n",
       "   'story',\n",
       "   ',',\n",
       "   'said',\n",
       "   'brian',\n",
       "   'russell',\n",
       "   ',',\n",
       "   'a',\n",
       "   'forensic',\n",
       "   'psychologist',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['psychology',\n",
       "   'can',\n",
       "   'explain',\n",
       "   'why',\n",
       "   'somebody',\n",
       "   'would',\n",
       "   'turn',\n",
       "   'rage',\n",
       "   'inward',\n",
       "   'on',\n",
       "   'themselves',\n",
       "   'about',\n",
       "   'the',\n",
       "   'fact',\n",
       "   'that',\n",
       "   'maybe',\n",
       "   'they',\n",
       "   'were',\n",
       "   \"n't\",\n",
       "   'going',\n",
       "   'to',\n",
       "   'keep',\n",
       "   'doing',\n",
       "   'their',\n",
       "   'job',\n",
       "   'and',\n",
       "   'they',\n",
       "   \"'re\",\n",
       "   'upset',\n",
       "   'about',\n",
       "   'that',\n",
       "   'and',\n",
       "   'so',\n",
       "   'they',\n",
       "   \"'re\",\n",
       "   'suicidal',\n",
       "   ',',\n",
       "   '``',\n",
       "   'he',\n",
       "   'said',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['but',\n",
       "   'there',\n",
       "   'is',\n",
       "   'no',\n",
       "   'mental',\n",
       "   'illness',\n",
       "   'that',\n",
       "   'explains',\n",
       "   'why',\n",
       "   'somebody',\n",
       "   'then',\n",
       "   'feels',\n",
       "   'entitled',\n",
       "   'to',\n",
       "   'also',\n",
       "   'take',\n",
       "   'that',\n",
       "   'rage',\n",
       "   'and',\n",
       "   'turn',\n",
       "   'it',\n",
       "   'outward',\n",
       "   'on',\n",
       "   '149',\n",
       "   'other',\n",
       "   'people',\n",
       "   'who',\n",
       "   'had',\n",
       "   'nothing',\n",
       "   'to',\n",
       "   'do',\n",
       "   'with',\n",
       "   'the',\n",
       "   'person',\n",
       "   \"'s\",\n",
       "   'problems',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['germanwings', 'crash', 'compensation', ':', 'what', 'we', 'know', '.'],\n",
       "  ['who', 'was', 'the', 'captain', 'of', 'germanwings', 'flight', '9525', '?'],\n",
       "  ['cnn',\n",
       "   \"'s\",\n",
       "   'margot',\n",
       "   'haddad',\n",
       "   'reported',\n",
       "   'from',\n",
       "   'marseille',\n",
       "   'and',\n",
       "   'pamela',\n",
       "   'brown',\n",
       "   'from',\n",
       "   'dusseldorf',\n",
       "   ',',\n",
       "   'while',\n",
       "   'laura',\n",
       "   'smith-spark',\n",
       "   'wrote',\n",
       "   'from',\n",
       "   'london',\n",
       "   '.'],\n",
       "  ['cnn',\n",
       "   \"'s\",\n",
       "   'frederik',\n",
       "   'pleitgen',\n",
       "   ',',\n",
       "   'pamela',\n",
       "   'boykoff',\n",
       "   ',',\n",
       "   'antonia',\n",
       "   'mortensen',\n",
       "   ',',\n",
       "   'sandrine',\n",
       "   'amiel',\n",
       "   'and',\n",
       "   'anna-maja',\n",
       "   'rappard',\n",
       "   'contributed',\n",
       "   'to',\n",
       "   'this',\n",
       "   'report',\n",
       "   '.']],\n",
       " 'tgt': [['marseille',\n",
       "   'prosecutor',\n",
       "   'says',\n",
       "   '``',\n",
       "   'so',\n",
       "   'far',\n",
       "   'no',\n",
       "   'videos',\n",
       "   'were',\n",
       "   'used',\n",
       "   'in',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'investigation',\n",
       "   '``',\n",
       "   'despite',\n",
       "   'media',\n",
       "   'reports',\n",
       "   '.'],\n",
       "  ['journalists',\n",
       "   'at',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'are',\n",
       "   '``',\n",
       "   'very',\n",
       "   'confident',\n",
       "   '``',\n",
       "   'the',\n",
       "   'video',\n",
       "   'clip',\n",
       "   'is',\n",
       "   'real',\n",
       "   ',',\n",
       "   'an',\n",
       "   'editor',\n",
       "   'says',\n",
       "   '.'],\n",
       "  ['andreas',\n",
       "   'lubitz',\n",
       "   'had',\n",
       "   'informed',\n",
       "   'his',\n",
       "   'lufthansa',\n",
       "   'training',\n",
       "   'school',\n",
       "   'of',\n",
       "   'an',\n",
       "   'episode',\n",
       "   'of',\n",
       "   'severe',\n",
       "   'depression',\n",
       "   ',',\n",
       "   'airline',\n",
       "   'says',\n",
       "   '.'],\n",
       "  []]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(PROCESSED_TRAIN_FILE)\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1006,\n",
       " 13229,\n",
       " 1007,\n",
       " 1011,\n",
       " 1011,\n",
       " 1996,\n",
       " 2120,\n",
       " 2374,\n",
       " 2223,\n",
       " 2038,\n",
       " 20733,\n",
       " 6731,\n",
       " 5865,\n",
       " 14929,\n",
       " 9074,\n",
       " 2745,\n",
       " 10967,\n",
       " 2243,\n",
       " 2302,\n",
       " 3477,\n",
       " 1010,\n",
       " 4584,\n",
       " 2007,\n",
       " 1996,\n",
       " 2223,\n",
       " 2056,\n",
       " 5958,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 5088,\n",
       " 2732,\n",
       " 2745,\n",
       " 10967,\n",
       " 2243,\n",
       " 2003,\n",
       " 2275,\n",
       " 2000,\n",
       " 3711,\n",
       " 1999,\n",
       " 2457,\n",
       " 6928,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1037,\n",
       " 3648,\n",
       " 2097,\n",
       " 2031,\n",
       " 1996,\n",
       " 2345,\n",
       " 2360,\n",
       " 2006,\n",
       " 1037,\n",
       " 14865,\n",
       " 3066,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 3041,\n",
       " 1010,\n",
       " 10967,\n",
       " 2243,\n",
       " 4914,\n",
       " 2000,\n",
       " 8019,\n",
       " 1999,\n",
       " 1037,\n",
       " 3899,\n",
       " 22158,\n",
       " 3614,\n",
       " 2004,\n",
       " 2112,\n",
       " 1997,\n",
       " 1037,\n",
       " 14865,\n",
       " 3820,\n",
       " 2007,\n",
       " 2976,\n",
       " 19608,\n",
       " 1999,\n",
       " 3448,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 2115,\n",
       " 4914,\n",
       " 6204,\n",
       " 2001,\n",
       " 2025,\n",
       " 2069,\n",
       " 6206,\n",
       " 1010,\n",
       " 2021,\n",
       " 2036,\n",
       " 10311,\n",
       " 1998,\n",
       " 16360,\n",
       " 2890,\n",
       " 10222,\n",
       " 19307,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2115,\n",
       " 2136,\n",
       " 1010,\n",
       " 1996,\n",
       " 5088,\n",
       " 1010,\n",
       " 1998,\n",
       " 5088,\n",
       " 4599,\n",
       " 2031,\n",
       " 2035,\n",
       " 2042,\n",
       " 3480,\n",
       " 2011,\n",
       " 2115,\n",
       " 4506,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 5088,\n",
       " 5849,\n",
       " 5074,\n",
       " 2204,\n",
       " 5349,\n",
       " 2056,\n",
       " 1999,\n",
       " 1037,\n",
       " 3661,\n",
       " 2000,\n",
       " 10967,\n",
       " 2243,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2204,\n",
       " 5349,\n",
       " 2056,\n",
       " 2002,\n",
       " 2052,\n",
       " 3319,\n",
       " 1996,\n",
       " 3570,\n",
       " 1997,\n",
       " 1996,\n",
       " 8636,\n",
       " 2044,\n",
       " 1996,\n",
       " 3423,\n",
       " 8931,\n",
       " 2024,\n",
       " 2058,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1999,\n",
       " 4981,\n",
       " 6406,\n",
       " 5958,\n",
       " 2007,\n",
       " 1037,\n",
       " 2976,\n",
       " 2457,\n",
       " 1999,\n",
       " 3448,\n",
       " 1010,\n",
       " 10967,\n",
       " 2243,\n",
       " 2036,\n",
       " 4914,\n",
       " 2008,\n",
       " 2002,\n",
       " 1998,\n",
       " 2048,\n",
       " 2522,\n",
       " 1011,\n",
       " 9530,\n",
       " 13102,\n",
       " 7895,\n",
       " 6591,\n",
       " 2730,\n",
       " 6077,\n",
       " 2008,\n",
       " 2106,\n",
       " 2025,\n",
       " 2954,\n",
       " 2092,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 14929,\n",
       " 3954,\n",
       " 4300,\n",
       " 8744,\n",
       " 2056,\n",
       " 10967,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 20247,\n",
       " 6235,\n",
       " 4506,\n",
       " 2008,\n",
       " 2024,\n",
       " 1036,\n",
       " 1036,\n",
       " 4297,\n",
       " 25377,\n",
       " 2890,\n",
       " 10222,\n",
       " 19307,\n",
       " 1998,\n",
       " 21873,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 8636,\n",
       " 3084,\n",
       " 1036,\n",
       " 1036,\n",
       " 1037,\n",
       " 2844,\n",
       " 4861,\n",
       " 2008,\n",
       " 6204,\n",
       " 2029,\n",
       " 16985,\n",
       " 24014,\n",
       " 2229,\n",
       " 1996,\n",
       " 2204,\n",
       " 5891,\n",
       " 1997,\n",
       " 1996,\n",
       " 5088,\n",
       " 2097,\n",
       " 2025,\n",
       " 2022,\n",
       " 25775,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 2002,\n",
       " 2056,\n",
       " 1999,\n",
       " 1037,\n",
       " 4861,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 3422,\n",
       " 2054,\n",
       " 2419,\n",
       " 2000,\n",
       " 10967,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 8636,\n",
       " 1036,\n",
       " 1036,\n",
       " 2204,\n",
       " 5349,\n",
       " 2056,\n",
       " 1996,\n",
       " 14929,\n",
       " 2071,\n",
       " 1036,\n",
       " 1036,\n",
       " 20865,\n",
       " 2151,\n",
       " 4447,\n",
       " 2030,\n",
       " 2128,\n",
       " 7583,\n",
       " 3111,\n",
       " 1036,\n",
       " 1036,\n",
       " 2000,\n",
       " 8980,\n",
       " 1002,\n",
       " 2570,\n",
       " 2454,\n",
       " 1997,\n",
       " 10967,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 6608,\n",
       " 6781,\n",
       " 2013,\n",
       " 1996,\n",
       " 2184,\n",
       " 1011,\n",
       " 2095,\n",
       " 1010,\n",
       " 1002,\n",
       " 7558,\n",
       " 2454,\n",
       " 3206,\n",
       " 2002,\n",
       " 2772,\n",
       " 1999,\n",
       " 2432,\n",
       " 1010,\n",
       " 2429,\n",
       " 2000,\n",
       " 1996,\n",
       " 3378,\n",
       " 2811,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 10967,\n",
       " 2243,\n",
       " 2056,\n",
       " 2002,\n",
       " 2052,\n",
       " 25803,\n",
       " 5905,\n",
       " 2000,\n",
       " 2028,\n",
       " 4175,\n",
       " 1997,\n",
       " 1036,\n",
       " 1036,\n",
       " 9714,\n",
       " 2000,\n",
       " 3604,\n",
       " 1999,\n",
       " 7553,\n",
       " 6236,\n",
       " 1999,\n",
       " 4681,\n",
       " 1997,\n",
       " 22300,\n",
       " 3450,\n",
       " 1998,\n",
       " 2000,\n",
       " 10460,\n",
       " 1037,\n",
       " 3899,\n",
       " 1999,\n",
       " 2019,\n",
       " 4111,\n",
       " 3554,\n",
       " 6957,\n",
       " 1036,\n",
       " 1036,\n",
       " 1999,\n",
       " 1037,\n",
       " 14865,\n",
       " 3820,\n",
       " 6406,\n",
       " 2012,\n",
       " 1057,\n",
       " 1012,\n",
       " 1055,\n",
       " 1012,\n",
       " 2212,\n",
       " 2457,\n",
       " 1999,\n",
       " 6713,\n",
       " 1010,\n",
       " 3448,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 3715,\n",
       " 2003,\n",
       " 16385,\n",
       " 3085,\n",
       " 2011,\n",
       " 2039,\n",
       " 2000,\n",
       " 2274,\n",
       " 2086,\n",
       " 1999,\n",
       " 3827,\n",
       " 1010,\n",
       " 1037,\n",
       " 1002,\n",
       " 5539,\n",
       " 1010,\n",
       " 2199,\n",
       " 2986,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 2440,\n",
       " 2717,\n",
       " 4183,\n",
       " 13700,\n",
       " 1010,\n",
       " 1037,\n",
       " 2569,\n",
       " 7667,\n",
       " 1998,\n",
       " 1017,\n",
       " 2086,\n",
       " 1997,\n",
       " 13588,\n",
       " 2713,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 1996,\n",
       " 14865,\n",
       " 3066,\n",
       " 2056,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2976,\n",
       " 19608,\n",
       " 3530,\n",
       " 2000,\n",
       " 3198,\n",
       " 2005,\n",
       " 1996,\n",
       " 2659,\n",
       " 2203,\n",
       " 1997,\n",
       " 1996,\n",
       " 23280,\n",
       " 11594,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 13474,\n",
       " 2097,\n",
       " 25803,\n",
       " 5905,\n",
       " 2138,\n",
       " 1996,\n",
       " 13474,\n",
       " 2003,\n",
       " 1999,\n",
       " 2755,\n",
       " 5905,\n",
       " 1997,\n",
       " 1996,\n",
       " 5338,\n",
       " 10048,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 1996,\n",
       " 14865,\n",
       " 3820,\n",
       " 2056,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1999,\n",
       " 2019,\n",
       " 3176,\n",
       " 12654,\n",
       " 1997,\n",
       " 8866,\n",
       " 1010,\n",
       " 2772,\n",
       " 2011,\n",
       " 10967,\n",
       " 2243,\n",
       " 1998,\n",
       " 6406,\n",
       " 2007,\n",
       " 1996,\n",
       " 3820,\n",
       " 1010,\n",
       " 10967,\n",
       " 2243,\n",
       " 4914,\n",
       " 9343,\n",
       " 6770,\n",
       " 12065,\n",
       " 1998,\n",
       " 1996,\n",
       " 3200,\n",
       " 2109,\n",
       " 2005,\n",
       " 2731,\n",
       " 1998,\n",
       " 3554,\n",
       " 1996,\n",
       " 6077,\n",
       " 1010,\n",
       " 2021,\n",
       " 1996,\n",
       " 4861,\n",
       " 2056,\n",
       " 2002,\n",
       " 2106,\n",
       " 2025,\n",
       " 6655,\n",
       " 2006,\n",
       " 1996,\n",
       " 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new : nfl chief , atlanta falcons owner critical of michael vick 's conduct .<q>nfl suspends falcons quarterback indefinitely without pay .<q>vick admits funding dogfighting operation but says he did not gamble .<q>vick due in federal court monday ; future in nfl remains uncertain .<q>\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( cnn ) -- the national football league has indefinitely suspended atlanta falcons quarterback michael vick without pay , officials with the league said friday .',\n",
       " 'nfl star michael vick is set to appear in court monday .',\n",
       " 'a judge will have the final say on a plea deal .',\n",
       " 'earlier , vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in virginia . ``',\n",
       " 'your admitted conduct was not only illegal , but also cruel and reprehensible .',\n",
       " 'your team , the nfl , and nfl fans have all been hurt by your actions , `` nfl commissioner roger goodell said in a letter to vick .',\n",
       " 'goodell said he would review the status of the suspension after the legal proceedings are over .',\n",
       " 'in papers filed friday with a federal court in virginia , vick also admitted that he and two co-conspirators killed dogs that did not fight well .',\n",
       " \"falcons owner arthur blank said vick 's admissions describe actions that are `` incomprehensible and unacceptable . ``\",\n",
       " 'the suspension makes `` a strong statement that conduct which tarnishes the good reputation of the nfl will not be tolerated , `` he said in a statement .',\n",
       " \"watch what led to vick 's suspension `` goodell said the falcons could `` assert any claims or remedies `` to recover $ 22 million of vick 's signing bonus from the 10-year , $ 130 million contract he signed in 2004 , according to the associated press .\",\n",
       " 'vick said he would plead guilty to one count of `` conspiracy to travel in interstate commerce in aid of unlawful activities and to sponsor a dog in an animal fighting venture `` in a plea agreement filed at u.s. district court in richmond , virginia .',\n",
       " 'the charge is punishable by up to five years in prison , a $ 250,000 fine , `` full restitution , a special assessment and 3 years of supervised release , `` the plea deal said .',\n",
       " 'federal prosecutors agreed to ask for the low end of the sentencing guidelines . ``',\n",
       " 'the defendant will plead guilty because the defendant is in fact guilty of the charged offense , `` the plea agreement said .',\n",
       " 'in an additional summary of facts , signed by vick and filed with the agreement , vick admitted buying pit bulls and the property used for training and fighting the dogs , but the statement said he did not bet on the fights or receive any of the money won . ``',\n",
       " \"most of the ` bad newz kennels ' operations and gambling monies were provided by vick , `` the official summary of facts said .\",\n",
       " 'gambling wins were generally split among co-conspirators tony taylor , quanis phillips and sometimes purnell peace , it continued . ``',\n",
       " 'vick did not gamble by placing side bets on any of the fights .',\n",
       " \"vick did not receive any of the proceeds from the purses that were won by ` bad newz kennels . '\",\n",
       " '`` vick also agreed that `` collective efforts `` by him and two others caused the deaths of at least six dogs .',\n",
       " \"around april , vick , peace and phillips tested some dogs in fighting sessions at vick 's property in virginia , the statement said . ``\",\n",
       " \"peace , phillips and vick agreed to the killing of approximately 6-8 dogs that did not perform well in ` testing ' sessions at 1915 moonlight road and all of those dogs were killed by various methods , including hanging and drowning . ``\",\n",
       " 'vick agrees and stipulates that these dogs all died as a result of the collective efforts of peace , phillips and vick , `` the summary said .',\n",
       " 'peace , 35 , of virginia beach , virginia ; phillips , 28 , of atlanta , georgia ; and taylor , 34 , of hampton , virginia , already have accepted agreements to plead guilty in exchange for reduced sentences .',\n",
       " 'vick , 27 , is scheduled to appear monday in court , where he is expected to plead guilty before a judge .',\n",
       " 'see a timeline of the case against vick `` the judge in the case will have the final say over the plea agreement .',\n",
       " \"the federal case against vick focused on the interstate conspiracy , but vick 's admission that he was involved in the killing of dogs could lead to local charges , according to cnn legal analyst jeffrey toobin . ``\",\n",
       " 'it sometimes happens -- not often -- that the state will follow a federal prosecution by charging its own crimes for exactly the same behavior , `` toobin said friday . ``',\n",
       " 'the risk for vick is , if he makes admissions in his federal guilty plea , the state of virginia could say , ` hey , look , you admitted violating virginia state law as well .',\n",
       " \"we 're going to introduce that against you and charge you in our court . '\",\n",
       " '`` in the plea deal , vick agreed to cooperate with investigators and provide all information he may have on any criminal activity and to testify if necessary .',\n",
       " 'vick also agreed to turn over any documents he has and to submit to polygraph tests .',\n",
       " 'vick agreed to `` make restitution for the full amount of the costs associated `` with the dogs that are being held by the government . ``',\n",
       " 'such costs may include , but are not limited to , all costs associated with the care of the dogs involved in that case , including if necessary , the long-term care and/or the humane euthanasia of some or all of those animals . ``',\n",
       " 'prosecutors , with the support of animal rights activists , have asked for permission to euthanize the dogs .',\n",
       " 'but the dogs could serve as important evidence in the cases against vick and his admitted co-conspirators .',\n",
       " 'judge henry e. hudson issued an order thursday telling the u.s. marshals service to `` arrest and seize the defendant property , and use discretion and whatever means appropriate to protect and maintain said defendant property . ``',\n",
       " \"both the judge 's order and vick 's filing refer to `` approximately `` 53 pit bull dogs .\",\n",
       " \"after vick 's indictment last month , goodell ordered the quarterback not to report to the falcons training camp , and the league is reviewing the case .\",\n",
       " \"blank told the nfl network on monday he could not speculate on vick 's future as a falcon , at least not until he had seen `` a statement of facts `` in the case .\",\n",
       " \"cnn 's mike phelan contributed to this report .\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['src_txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of BertSumExtractiveSummarizer, a wrapper for running BertSum-based finetuning. You can select any device ID on your machine, but make sure that you include the string version of the device ID in the gpu_ranks argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose which GPU device to use\n",
    "device_id = 0\n",
    "gpu_ranks = str(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the encoder algorithm. There are four options:\n",
    "- baseline: it used a smaller transformer model to replace the bert model and with transformer summarization layer\n",
    "- classifier: it uses pretrained BERT and fine-tune BERT with **simple logistic classification** summarization layer\n",
    "- transformer: it uses pretrained BERT and fine-tune BERT with **transformer** summarization layer\n",
    "- RNN: it uses pretrained BERT and fine-tune BERT with **LSTM** summarization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = 'transformer'\n",
    "model_base_path = './models/'\n",
    "log_base_path = './logs/'\n",
    "result_base_path = './results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(model_base_path):\n",
    "    os.makedirs(model_base_path)\n",
    "if not os.path.exists(log_base_path):\n",
    "    os.makedirs(log_base_path)\n",
    "if not os.path.exists(result_base_path):\n",
    "    os.makedirs(result_base_path)\n",
    "    \n",
    "from random import random\n",
    "random_number = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n",
      "{0: 0}\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import BertSumExtractiveSummarizer\n",
    "bertsum_model = BertSumExtractiveSummarizer(encoder = encoder, \n",
    "                                            model_path = model_base_path + encoder + str(random_number),\n",
    "                                            log_file = log_base_path + encoder + str(random_number),\n",
    "                                            bert_config_path = BERT_CONFIG_PATH,\n",
    "                                            device_id = device_id,\n",
    "                                            gpu_ranks = gpu_ranks,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the fully processed CNN/DM dataset to train the model. During the training, you can stop any time and retrain from the previous saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PREPROCESSED_DATA is False:\n",
    "    training_data_files = [PROCESSED_TRAIN_FILE]\n",
    "else:    \n",
    "    import glob\n",
    "    pts = sorted(glob.glob(BERT_DATA_PATH + 'cnndm.train' + '.[0-9]*.pt'))\n",
    "    training_data_files = pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./bert_data/cnndm.train.0.bert.pt',\n",
       " './bert_data/cnndm.train.1.bert.pt',\n",
       " './bert_data/cnndm.train.10.bert.pt',\n",
       " './bert_data/cnndm.train.100.bert.pt',\n",
       " './bert_data/cnndm.train.101.bert.pt',\n",
       " './bert_data/cnndm.train.102.bert.pt',\n",
       " './bert_data/cnndm.train.103.bert.pt',\n",
       " './bert_data/cnndm.train.104.bert.pt',\n",
       " './bert_data/cnndm.train.105.bert.pt',\n",
       " './bert_data/cnndm.train.106.bert.pt',\n",
       " './bert_data/cnndm.train.107.bert.pt',\n",
       " './bert_data/cnndm.train.108.bert.pt',\n",
       " './bert_data/cnndm.train.109.bert.pt',\n",
       " './bert_data/cnndm.train.11.bert.pt',\n",
       " './bert_data/cnndm.train.110.bert.pt',\n",
       " './bert_data/cnndm.train.111.bert.pt',\n",
       " './bert_data/cnndm.train.112.bert.pt',\n",
       " './bert_data/cnndm.train.113.bert.pt',\n",
       " './bert_data/cnndm.train.114.bert.pt',\n",
       " './bert_data/cnndm.train.115.bert.pt',\n",
       " './bert_data/cnndm.train.116.bert.pt',\n",
       " './bert_data/cnndm.train.117.bert.pt',\n",
       " './bert_data/cnndm.train.118.bert.pt',\n",
       " './bert_data/cnndm.train.119.bert.pt',\n",
       " './bert_data/cnndm.train.12.bert.pt',\n",
       " './bert_data/cnndm.train.120.bert.pt',\n",
       " './bert_data/cnndm.train.121.bert.pt',\n",
       " './bert_data/cnndm.train.122.bert.pt',\n",
       " './bert_data/cnndm.train.123.bert.pt',\n",
       " './bert_data/cnndm.train.124.bert.pt',\n",
       " './bert_data/cnndm.train.125.bert.pt',\n",
       " './bert_data/cnndm.train.126.bert.pt',\n",
       " './bert_data/cnndm.train.127.bert.pt',\n",
       " './bert_data/cnndm.train.128.bert.pt',\n",
       " './bert_data/cnndm.train.129.bert.pt',\n",
       " './bert_data/cnndm.train.13.bert.pt',\n",
       " './bert_data/cnndm.train.130.bert.pt',\n",
       " './bert_data/cnndm.train.131.bert.pt',\n",
       " './bert_data/cnndm.train.132.bert.pt',\n",
       " './bert_data/cnndm.train.133.bert.pt',\n",
       " './bert_data/cnndm.train.134.bert.pt',\n",
       " './bert_data/cnndm.train.135.bert.pt',\n",
       " './bert_data/cnndm.train.136.bert.pt',\n",
       " './bert_data/cnndm.train.137.bert.pt',\n",
       " './bert_data/cnndm.train.138.bert.pt',\n",
       " './bert_data/cnndm.train.139.bert.pt',\n",
       " './bert_data/cnndm.train.14.bert.pt',\n",
       " './bert_data/cnndm.train.140.bert.pt',\n",
       " './bert_data/cnndm.train.141.bert.pt',\n",
       " './bert_data/cnndm.train.142.bert.pt',\n",
       " './bert_data/cnndm.train.143.bert.pt',\n",
       " './bert_data/cnndm.train.15.bert.pt',\n",
       " './bert_data/cnndm.train.16.bert.pt',\n",
       " './bert_data/cnndm.train.17.bert.pt',\n",
       " './bert_data/cnndm.train.18.bert.pt',\n",
       " './bert_data/cnndm.train.19.bert.pt',\n",
       " './bert_data/cnndm.train.2.bert.pt',\n",
       " './bert_data/cnndm.train.20.bert.pt',\n",
       " './bert_data/cnndm.train.21.bert.pt',\n",
       " './bert_data/cnndm.train.22.bert.pt',\n",
       " './bert_data/cnndm.train.23.bert.pt',\n",
       " './bert_data/cnndm.train.24.bert.pt',\n",
       " './bert_data/cnndm.train.25.bert.pt',\n",
       " './bert_data/cnndm.train.26.bert.pt',\n",
       " './bert_data/cnndm.train.27.bert.pt',\n",
       " './bert_data/cnndm.train.28.bert.pt',\n",
       " './bert_data/cnndm.train.29.bert.pt',\n",
       " './bert_data/cnndm.train.3.bert.pt',\n",
       " './bert_data/cnndm.train.30.bert.pt',\n",
       " './bert_data/cnndm.train.31.bert.pt',\n",
       " './bert_data/cnndm.train.32.bert.pt',\n",
       " './bert_data/cnndm.train.33.bert.pt',\n",
       " './bert_data/cnndm.train.34.bert.pt',\n",
       " './bert_data/cnndm.train.35.bert.pt',\n",
       " './bert_data/cnndm.train.36.bert.pt',\n",
       " './bert_data/cnndm.train.37.bert.pt',\n",
       " './bert_data/cnndm.train.38.bert.pt',\n",
       " './bert_data/cnndm.train.39.bert.pt',\n",
       " './bert_data/cnndm.train.4.bert.pt',\n",
       " './bert_data/cnndm.train.40.bert.pt',\n",
       " './bert_data/cnndm.train.41.bert.pt',\n",
       " './bert_data/cnndm.train.42.bert.pt',\n",
       " './bert_data/cnndm.train.43.bert.pt',\n",
       " './bert_data/cnndm.train.44.bert.pt',\n",
       " './bert_data/cnndm.train.45.bert.pt',\n",
       " './bert_data/cnndm.train.46.bert.pt',\n",
       " './bert_data/cnndm.train.47.bert.pt',\n",
       " './bert_data/cnndm.train.48.bert.pt',\n",
       " './bert_data/cnndm.train.49.bert.pt',\n",
       " './bert_data/cnndm.train.5.bert.pt',\n",
       " './bert_data/cnndm.train.50.bert.pt',\n",
       " './bert_data/cnndm.train.51.bert.pt',\n",
       " './bert_data/cnndm.train.52.bert.pt',\n",
       " './bert_data/cnndm.train.53.bert.pt',\n",
       " './bert_data/cnndm.train.54.bert.pt',\n",
       " './bert_data/cnndm.train.55.bert.pt',\n",
       " './bert_data/cnndm.train.56.bert.pt',\n",
       " './bert_data/cnndm.train.57.bert.pt',\n",
       " './bert_data/cnndm.train.58.bert.pt',\n",
       " './bert_data/cnndm.train.59.bert.pt',\n",
       " './bert_data/cnndm.train.6.bert.pt',\n",
       " './bert_data/cnndm.train.60.bert.pt',\n",
       " './bert_data/cnndm.train.61.bert.pt',\n",
       " './bert_data/cnndm.train.62.bert.pt',\n",
       " './bert_data/cnndm.train.63.bert.pt',\n",
       " './bert_data/cnndm.train.64.bert.pt',\n",
       " './bert_data/cnndm.train.65.bert.pt',\n",
       " './bert_data/cnndm.train.66.bert.pt',\n",
       " './bert_data/cnndm.train.67.bert.pt',\n",
       " './bert_data/cnndm.train.68.bert.pt',\n",
       " './bert_data/cnndm.train.69.bert.pt',\n",
       " './bert_data/cnndm.train.7.bert.pt',\n",
       " './bert_data/cnndm.train.70.bert.pt',\n",
       " './bert_data/cnndm.train.71.bert.pt',\n",
       " './bert_data/cnndm.train.72.bert.pt',\n",
       " './bert_data/cnndm.train.73.bert.pt',\n",
       " './bert_data/cnndm.train.74.bert.pt',\n",
       " './bert_data/cnndm.train.75.bert.pt',\n",
       " './bert_data/cnndm.train.76.bert.pt',\n",
       " './bert_data/cnndm.train.77.bert.pt',\n",
       " './bert_data/cnndm.train.78.bert.pt',\n",
       " './bert_data/cnndm.train.79.bert.pt',\n",
       " './bert_data/cnndm.train.8.bert.pt',\n",
       " './bert_data/cnndm.train.80.bert.pt',\n",
       " './bert_data/cnndm.train.81.bert.pt',\n",
       " './bert_data/cnndm.train.82.bert.pt',\n",
       " './bert_data/cnndm.train.83.bert.pt',\n",
       " './bert_data/cnndm.train.84.bert.pt',\n",
       " './bert_data/cnndm.train.85.bert.pt',\n",
       " './bert_data/cnndm.train.86.bert.pt',\n",
       " './bert_data/cnndm.train.87.bert.pt',\n",
       " './bert_data/cnndm.train.88.bert.pt',\n",
       " './bert_data/cnndm.train.89.bert.pt',\n",
       " './bert_data/cnndm.train.9.bert.pt',\n",
       " './bert_data/cnndm.train.90.bert.pt',\n",
       " './bert_data/cnndm.train.91.bert.pt',\n",
       " './bert_data/cnndm.train.92.bert.pt',\n",
       " './bert_data/cnndm.train.93.bert.pt',\n",
       " './bert_data/cnndm.train.94.bert.pt',\n",
       " './bert_data/cnndm.train.95.bert.pt',\n",
       " './bert_data/cnndm.train.96.bert.pt',\n",
       " './bert_data/cnndm.train.97.bert.pt',\n",
       " './bert_data/cnndm.train.98.bert.pt',\n",
       " './bert_data/cnndm.train.99.bert.pt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training_steps is (number of epoches * the total number of batches in the training data )/ accumulation_steps \n",
    "## batch_size is the maximum number of tokens among all the training examples * number of training examples,\n",
    "## training steps used by each GPU should be divided by number of GPU used for fair comparison.\n",
    "## usually 10K steps can yield a model with decent performance\n",
    "if QUICK_RUN:\n",
    "    train_steps = 10000\n",
    "else:\n",
    "    train_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 17:30:45,152 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "[2019-10-15 17:30:45,154 INFO] extracting archive file ./temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpjp15yzc0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accum_count': 2, 'batch_size': 3000, 'beta1': 0.9, 'beta2': 0.999, 'block_trigram': True, 'decay_method': 'noam', 'dropout': 0.1, 'encoder': 'transformer', 'ff_size': 512, 'gpu_ranks': '0', 'heads': 4, 'hidden_size': 128, 'inter_layers': 2, 'lr': 0.002, 'max_grad_norm': 0, 'max_nsents': 100, 'max_src_ntokens': 200, 'min_nsents': 3, 'min_src_ntokens': 10, 'optim': 'adam', 'oracle_mode': 'combination', 'param_init': 0.0, 'param_init_glorot': True, 'recall_eval': False, 'report_every': 50, 'report_rouge': True, 'rnn_size': 512, 'save_checkpoint_steps': 500, 'seed': 42, 'temp_dir': './temp', 'test_all': False, 'test_from': '', 'train_from': '', 'use_interval': True, 'visible_gpus': '0', 'warmup_steps': 2000, 'world_size': 1, 'mode': 'train', 'model_path': './models/transformer0.37907517824181713', 'log_file': './logs/transformer0.37907517824181713', 'bert_config_path': './bert_config_uncased_base.json', 'gpu_ranks_map': {0: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 17:30:48,988 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2019-10-15 17:30:54,399 INFO] * number of parameters: 115790849\n",
      "[2019-10-15 17:30:54,400 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 0\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 17:31:15,819 INFO] Step 50/10000; xent: 3.89; lr: 0.0000011;  48 docs/s;     21 sec\n",
      "[2019-10-15 17:31:36,569 INFO] Step 100/10000; xent: 3.29; lr: 0.0000022;  49 docs/s;     42 sec\n",
      "[2019-10-15 17:31:57,342 INFO] Step 150/10000; xent: 3.29; lr: 0.0000034;  48 docs/s;     63 sec\n",
      "[2019-10-15 17:32:18,802 INFO] Step 200/10000; xent: 3.29; lr: 0.0000045;  47 docs/s;     84 sec\n",
      "[2019-10-15 17:32:39,486 INFO] Step 250/10000; xent: 3.22; lr: 0.0000056;  48 docs/s;    105 sec\n",
      "[2019-10-15 17:33:00,459 INFO] Step 300/10000; xent: 3.14; lr: 0.0000067;  48 docs/s;    126 sec\n",
      "[2019-10-15 17:33:21,471 INFO] Step 350/10000; xent: 3.19; lr: 0.0000078;  49 docs/s;    147 sec\n",
      "[2019-10-15 17:33:42,961 INFO] Step 400/10000; xent: 3.20; lr: 0.0000089;  46 docs/s;    168 sec\n",
      "[2019-10-15 17:34:03,756 INFO] Step 450/10000; xent: 3.22; lr: 0.0000101;  48 docs/s;    189 sec\n",
      "[2019-10-15 17:34:24,637 INFO] Step 500/10000; xent: 3.16; lr: 0.0000112;  48 docs/s;    210 sec\n",
      "[2019-10-15 17:34:24,643 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_500.pt\n",
      "[2019-10-15 17:34:46,851 INFO] Step 550/10000; xent: 3.19; lr: 0.0000123;  45 docs/s;    232 sec\n",
      "[2019-10-15 17:35:08,537 INFO] Step 600/10000; xent: 3.18; lr: 0.0000134;  46 docs/s;    254 sec\n",
      "[2019-10-15 17:35:29,358 INFO] Step 650/10000; xent: 3.11; lr: 0.0000145;  48 docs/s;    275 sec\n",
      "[2019-10-15 17:35:50,269 INFO] Step 700/10000; xent: 3.09; lr: 0.0000157;  48 docs/s;    295 sec\n",
      "[2019-10-15 17:36:11,107 INFO] Step 750/10000; xent: 3.06; lr: 0.0000168;  48 docs/s;    316 sec\n",
      "[2019-10-15 17:36:32,373 INFO] Step 800/10000; xent: 3.10; lr: 0.0000179;  47 docs/s;    338 sec\n",
      "[2019-10-15 17:36:53,196 INFO] Step 850/10000; xent: 3.01; lr: 0.0000190;  48 docs/s;    358 sec\n",
      "[2019-10-15 17:37:14,282 INFO] Step 900/10000; xent: 3.08; lr: 0.0000201;  48 docs/s;    379 sec\n",
      "[2019-10-15 17:37:34,972 INFO] Step 950/10000; xent: 2.99; lr: 0.0000212;  48 docs/s;    400 sec\n",
      "[2019-10-15 17:37:56,426 INFO] Step 1000/10000; xent: 3.02; lr: 0.0000224;  47 docs/s;    422 sec\n",
      "[2019-10-15 17:37:56,429 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_1000.pt\n",
      "[2019-10-15 17:38:18,737 INFO] Step 1050/10000; xent: 2.96; lr: 0.0000235;  46 docs/s;    444 sec\n",
      "[2019-10-15 17:38:39,847 INFO] Step 1100/10000; xent: 3.05; lr: 0.0000246;  47 docs/s;    465 sec\n",
      "[2019-10-15 17:39:00,668 INFO] Step 1150/10000; xent: 2.88; lr: 0.0000257;  49 docs/s;    486 sec\n",
      "[2019-10-15 17:39:22,145 INFO] Step 1200/10000; xent: 2.99; lr: 0.0000268;  47 docs/s;    507 sec\n",
      "[2019-10-15 17:39:42,943 INFO] Step 1250/10000; xent: 2.91; lr: 0.0000280;  48 docs/s;    528 sec\n",
      "[2019-10-15 17:40:03,759 INFO] Step 1300/10000; xent: 2.95; lr: 0.0000291;  49 docs/s;    549 sec\n",
      "[2019-10-15 17:40:24,449 INFO] Step 1350/10000; xent: 2.95; lr: 0.0000302;  48 docs/s;    570 sec\n",
      "[2019-10-15 17:40:45,976 INFO] Step 1400/10000; xent: 2.88; lr: 0.0000313;  47 docs/s;    591 sec\n",
      "[2019-10-15 17:41:06,835 INFO] Step 1450/10000; xent: 2.96; lr: 0.0000324;  48 docs/s;    612 sec\n",
      "[2019-10-15 17:41:27,558 INFO] Step 1500/10000; xent: 2.93; lr: 0.0000335;  48 docs/s;    633 sec\n",
      "[2019-10-15 17:41:27,563 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_1500.pt\n",
      "[2019-10-15 17:41:49,573 INFO] Step 1550/10000; xent: 2.98; lr: 0.0000347;  46 docs/s;    655 sec\n",
      "[2019-10-15 17:42:11,140 INFO] Step 1600/10000; xent: 2.95; lr: 0.0000358;  47 docs/s;    676 sec\n",
      "[2019-10-15 17:42:31,857 INFO] Step 1650/10000; xent: 2.90; lr: 0.0000369;  48 docs/s;    697 sec\n",
      "[2019-10-15 17:42:52,776 INFO] Step 1700/10000; xent: 2.88; lr: 0.0000380;  48 docs/s;    718 sec\n",
      "[2019-10-15 17:43:13,306 INFO] Step 1750/10000; xent: 2.90; lr: 0.0000391;  49 docs/s;    738 sec\n",
      "[2019-10-15 17:43:35,057 INFO] Step 1800/10000; xent: 2.85; lr: 0.0000402;  46 docs/s;    760 sec\n",
      "[2019-10-15 17:43:55,848 INFO] Step 1850/10000; xent: 3.02; lr: 0.0000414;  48 docs/s;    781 sec\n",
      "[2019-10-15 17:44:16,856 INFO] Step 1900/10000; xent: 2.94; lr: 0.0000425;  48 docs/s;    802 sec\n",
      "[2019-10-15 17:44:37,746 INFO] Step 1950/10000; xent: 2.86; lr: 0.0000436;  49 docs/s;    823 sec\n",
      "[2019-10-15 17:44:59,222 INFO] Step 2000/10000; xent: 2.88; lr: 0.0000447;  47 docs/s;    844 sec\n",
      "[2019-10-15 17:44:59,226 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_2000.pt\n",
      "[2019-10-15 17:45:21,420 INFO] Step 2050/10000; xent: 2.80; lr: 0.0000442;  45 docs/s;    867 sec\n",
      "[2019-10-15 17:45:42,134 INFO] Step 2100/10000; xent: 2.87; lr: 0.0000436;  48 docs/s;    887 sec\n",
      "[2019-10-15 17:46:03,052 INFO] Step 2150/10000; xent: 2.93; lr: 0.0000431;  48 docs/s;    908 sec\n",
      "[2019-10-15 17:46:24,419 INFO] Step 2200/10000; xent: 2.88; lr: 0.0000426;  47 docs/s;    930 sec\n",
      "[2019-10-15 17:46:45,179 INFO] Step 2250/10000; xent: 2.89; lr: 0.0000422;  48 docs/s;    950 sec\n",
      "[2019-10-15 17:47:06,010 INFO] Step 2300/10000; xent: 2.88; lr: 0.0000417;  48 docs/s;    971 sec\n",
      "[2019-10-15 17:47:26,863 INFO] Step 2350/10000; xent: 2.89; lr: 0.0000413;  49 docs/s;    992 sec\n",
      "[2019-10-15 17:47:48,436 INFO] Step 2400/10000; xent: 2.81; lr: 0.0000408;  47 docs/s;   1014 sec\n",
      "[2019-10-15 17:48:09,264 INFO] Step 2450/10000; xent: 2.78; lr: 0.0000404;  48 docs/s;   1034 sec\n",
      "[2019-10-15 17:48:30,174 INFO] Step 2500/10000; xent: 2.84; lr: 0.0000400;  49 docs/s;   1055 sec\n",
      "[2019-10-15 17:48:30,179 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_2500.pt\n",
      "[2019-10-15 17:48:52,271 INFO] Step 2550/10000; xent: 2.92; lr: 0.0000396;  46 docs/s;   1077 sec\n",
      "[2019-10-15 17:49:15,105 INFO] Step 2600/10000; xent: 2.77; lr: 0.0000392;  44 docs/s;   1100 sec\n",
      "[2019-10-15 17:49:35,998 INFO] Step 2650/10000; xent: 2.92; lr: 0.0000389;  48 docs/s;   1121 sec\n",
      "[2019-10-15 17:49:56,785 INFO] Step 2700/10000; xent: 2.95; lr: 0.0000385;  48 docs/s;   1142 sec\n",
      "[2019-10-15 17:50:17,612 INFO] Step 2750/10000; xent: 2.87; lr: 0.0000381;  48 docs/s;   1163 sec\n",
      "[2019-10-15 17:50:38,911 INFO] Step 2800/10000; xent: 2.80; lr: 0.0000378;  47 docs/s;   1184 sec\n",
      "[2019-10-15 17:50:59,692 INFO] Step 2850/10000; xent: 2.90; lr: 0.0000375;  48 docs/s;   1205 sec\n",
      "[2019-10-15 17:51:20,554 INFO] Step 2900/10000; xent: 2.90; lr: 0.0000371;  49 docs/s;   1226 sec\n",
      "[2019-10-15 17:51:41,323 INFO] Step 2950/10000; xent: 2.85; lr: 0.0000368;  49 docs/s;   1246 sec\n",
      "[2019-10-15 17:52:02,767 INFO] Step 3000/10000; xent: 2.88; lr: 0.0000365;  47 docs/s;   1268 sec\n",
      "[2019-10-15 17:52:02,771 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_3000.pt\n",
      "[2019-10-15 17:52:24,775 INFO] Step 3050/10000; xent: 2.91; lr: 0.0000362;  46 docs/s;   1290 sec\n",
      "[2019-10-15 17:52:45,578 INFO] Step 3100/10000; xent: 2.83; lr: 0.0000359;  48 docs/s;   1311 sec\n",
      "[2019-10-15 17:53:06,271 INFO] Step 3150/10000; xent: 2.89; lr: 0.0000356;  49 docs/s;   1331 sec\n",
      "[2019-10-15 17:53:27,922 INFO] Step 3200/10000; xent: 2.83; lr: 0.0000354;  47 docs/s;   1353 sec\n",
      "[2019-10-15 17:53:48,843 INFO] Step 3250/10000; xent: 2.84; lr: 0.0000351;  48 docs/s;   1374 sec\n",
      "[2019-10-15 17:54:09,673 INFO] Step 3300/10000; xent: 2.73; lr: 0.0000348;  48 docs/s;   1395 sec\n",
      "[2019-10-15 17:54:30,414 INFO] Step 3350/10000; xent: 2.85; lr: 0.0000346;  49 docs/s;   1416 sec\n",
      "[2019-10-15 17:54:51,856 INFO] Step 3400/10000; xent: 2.78; lr: 0.0000343;  47 docs/s;   1437 sec\n",
      "[2019-10-15 17:55:12,619 INFO] Step 3450/10000; xent: 2.83; lr: 0.0000341;  49 docs/s;   1458 sec\n",
      "[2019-10-15 17:55:33,266 INFO] Step 3500/10000; xent: 2.80; lr: 0.0000338;  49 docs/s;   1478 sec\n",
      "[2019-10-15 17:55:33,270 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_3500.pt\n",
      "[2019-10-15 17:55:55,239 INFO] Step 3550/10000; xent: 2.80; lr: 0.0000336;  46 docs/s;   1500 sec\n",
      "[2019-10-15 17:56:17,401 INFO] Step 3600/10000; xent: 2.76; lr: 0.0000333;  45 docs/s;   1523 sec\n",
      "[2019-10-15 17:56:38,129 INFO] Step 3650/10000; xent: 2.80; lr: 0.0000331;  48 docs/s;   1543 sec\n",
      "[2019-10-15 17:56:58,914 INFO] Step 3700/10000; xent: 2.83; lr: 0.0000329;  49 docs/s;   1564 sec\n",
      "[2019-10-15 17:57:19,588 INFO] Step 3750/10000; xent: 2.82; lr: 0.0000327;  49 docs/s;   1585 sec\n",
      "[2019-10-15 17:57:40,985 INFO] Step 3800/10000; xent: 2.91; lr: 0.0000324;  47 docs/s;   1606 sec\n",
      "[2019-10-15 17:58:01,759 INFO] Step 3850/10000; xent: 2.87; lr: 0.0000322;  49 docs/s;   1627 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 17:58:22,621 INFO] Step 3900/10000; xent: 2.79; lr: 0.0000320;  48 docs/s;   1648 sec\n",
      "[2019-10-15 17:58:43,322 INFO] Step 3950/10000; xent: 2.90; lr: 0.0000318;  49 docs/s;   1668 sec\n",
      "[2019-10-15 17:59:04,810 INFO] Step 4000/10000; xent: 2.80; lr: 0.0000316;  47 docs/s;   1690 sec\n",
      "[2019-10-15 17:59:04,814 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_4000.pt\n",
      "[2019-10-15 17:59:26,773 INFO] Step 4050/10000; xent: 2.89; lr: 0.0000314;  45 docs/s;   1712 sec\n",
      "[2019-10-15 17:59:47,458 INFO] Step 4100/10000; xent: 2.86; lr: 0.0000312;  49 docs/s;   1733 sec\n",
      "[2019-10-15 18:00:08,124 INFO] Step 4150/10000; xent: 2.87; lr: 0.0000310;  48 docs/s;   1753 sec\n",
      "[2019-10-15 18:00:29,734 INFO] Step 4200/10000; xent: 2.93; lr: 0.0000309;  47 docs/s;   1775 sec\n",
      "[2019-10-15 18:00:50,554 INFO] Step 4250/10000; xent: 2.86; lr: 0.0000307;  49 docs/s;   1796 sec\n",
      "[2019-10-15 18:01:11,237 INFO] Step 4300/10000; xent: 2.80; lr: 0.0000305;  49 docs/s;   1816 sec\n",
      "[2019-10-15 18:01:31,908 INFO] Step 4350/10000; xent: 2.87; lr: 0.0000303;  49 docs/s;   1837 sec\n",
      "[2019-10-15 18:01:53,271 INFO] Step 4400/10000; xent: 2.79; lr: 0.0000302;  47 docs/s;   1858 sec\n",
      "[2019-10-15 18:02:14,150 INFO] Step 4450/10000; xent: 2.80; lr: 0.0000300;  49 docs/s;   1879 sec\n",
      "[2019-10-15 18:02:34,861 INFO] Step 4500/10000; xent: 2.89; lr: 0.0000298;  48 docs/s;   1900 sec\n",
      "[2019-10-15 18:02:34,865 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_4500.pt\n",
      "[2019-10-15 18:02:56,909 INFO] Step 4550/10000; xent: 2.80; lr: 0.0000296;  46 docs/s;   1922 sec\n",
      "[2019-10-15 18:03:18,328 INFO] Step 4600/10000; xent: 2.81; lr: 0.0000295;  47 docs/s;   1943 sec\n",
      "[2019-10-15 18:03:39,125 INFO] Step 4650/10000; xent: 2.81; lr: 0.0000293;  48 docs/s;   1964 sec\n",
      "[2019-10-15 18:03:59,868 INFO] Step 4700/10000; xent: 2.87; lr: 0.0000292;  49 docs/s;   1985 sec\n",
      "[2019-10-15 18:04:20,701 INFO] Step 4750/10000; xent: 2.82; lr: 0.0000290;  48 docs/s;   2006 sec\n",
      "[2019-10-15 18:04:42,076 INFO] Step 4800/10000; xent: 2.82; lr: 0.0000289;  47 docs/s;   2027 sec\n",
      "[2019-10-15 18:05:02,834 INFO] Step 4850/10000; xent: 2.80; lr: 0.0000287;  48 docs/s;   2048 sec\n",
      "[2019-10-15 18:05:23,556 INFO] Step 4900/10000; xent: 2.81; lr: 0.0000286;  49 docs/s;   2069 sec\n",
      "[2019-10-15 18:05:44,221 INFO] Step 4950/10000; xent: 2.85; lr: 0.0000284;  49 docs/s;   2089 sec\n",
      "[2019-10-15 18:06:05,673 INFO] Step 5000/10000; xent: 2.78; lr: 0.0000283;  47 docs/s;   2111 sec\n",
      "[2019-10-15 18:06:05,677 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_5000.pt\n",
      "[2019-10-15 18:06:28,010 INFO] Step 5050/10000; xent: 2.84; lr: 0.0000281;  45 docs/s;   2133 sec\n",
      "[2019-10-15 18:06:48,974 INFO] Step 5100/10000; xent: 2.73; lr: 0.0000280;  48 docs/s;   2154 sec\n",
      "[2019-10-15 18:07:09,784 INFO] Step 5150/10000; xent: 2.85; lr: 0.0000279;  48 docs/s;   2175 sec\n",
      "[2019-10-15 18:07:31,104 INFO] Step 5200/10000; xent: 2.80; lr: 0.0000277;  47 docs/s;   2196 sec\n",
      "[2019-10-15 18:07:51,971 INFO] Step 5250/10000; xent: 2.90; lr: 0.0000276;  49 docs/s;   2217 sec\n",
      "[2019-10-15 18:08:12,738 INFO] Step 5300/10000; xent: 2.91; lr: 0.0000275;  48 docs/s;   2238 sec\n",
      "[2019-10-15 18:08:33,531 INFO] Step 5350/10000; xent: 2.76; lr: 0.0000273;  49 docs/s;   2259 sec\n",
      "[2019-10-15 18:08:54,887 INFO] Step 5400/10000; xent: 2.81; lr: 0.0000272;  47 docs/s;   2280 sec\n",
      "[2019-10-15 18:09:15,662 INFO] Step 5450/10000; xent: 2.81; lr: 0.0000271;  48 docs/s;   2301 sec\n",
      "[2019-10-15 18:09:36,458 INFO] Step 5500/10000; xent: 2.84; lr: 0.0000270;  48 docs/s;   2322 sec\n",
      "[2019-10-15 18:09:36,462 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_5500.pt\n",
      "[2019-10-15 18:09:58,522 INFO] Step 5550/10000; xent: 2.84; lr: 0.0000268;  46 docs/s;   2344 sec\n",
      "[2019-10-15 18:10:20,176 INFO] Step 5600/10000; xent: 2.86; lr: 0.0000267;  47 docs/s;   2365 sec\n",
      "[2019-10-15 18:10:40,957 INFO] Step 5650/10000; xent: 2.84; lr: 0.0000266;  48 docs/s;   2386 sec\n",
      "[2019-10-15 18:11:01,721 INFO] Step 5700/10000; xent: 2.73; lr: 0.0000265;  49 docs/s;   2407 sec\n",
      "[2019-10-15 18:11:22,585 INFO] Step 5750/10000; xent: 2.83; lr: 0.0000264;  48 docs/s;   2428 sec\n",
      "[2019-10-15 18:11:43,794 INFO] Step 5800/10000; xent: 2.78; lr: 0.0000263;  47 docs/s;   2449 sec\n",
      "[2019-10-15 18:12:04,659 INFO] Step 5850/10000; xent: 2.89; lr: 0.0000261;  48 docs/s;   2470 sec\n",
      "[2019-10-15 18:12:25,349 INFO] Step 5900/10000; xent: 2.85; lr: 0.0000260;  48 docs/s;   2491 sec\n",
      "[2019-10-15 18:12:46,081 INFO] Step 5950/10000; xent: 2.73; lr: 0.0000259;  49 docs/s;   2511 sec\n",
      "[2019-10-15 18:13:07,405 INFO] Step 6000/10000; xent: 2.82; lr: 0.0000258;  47 docs/s;   2533 sec\n",
      "[2019-10-15 18:13:07,409 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_6000.pt\n",
      "[2019-10-15 18:13:29,137 INFO] Step 6050/10000; xent: 2.85; lr: 0.0000257;  45 docs/s;   2554 sec\n",
      "[2019-10-15 18:13:49,752 INFO] Step 6100/10000; xent: 2.80; lr: 0.0000256;  48 docs/s;   2575 sec\n",
      "[2019-10-15 18:14:10,499 INFO] Step 6150/10000; xent: 2.80; lr: 0.0000255;  48 docs/s;   2596 sec\n",
      "[2019-10-15 18:14:31,939 INFO] Step 6200/10000; xent: 2.81; lr: 0.0000254;  47 docs/s;   2617 sec\n",
      "[2019-10-15 18:14:52,749 INFO] Step 6250/10000; xent: 2.80; lr: 0.0000253;  48 docs/s;   2638 sec\n",
      "[2019-10-15 18:15:13,515 INFO] Step 6300/10000; xent: 2.82; lr: 0.0000252;  48 docs/s;   2659 sec\n",
      "[2019-10-15 18:15:34,316 INFO] Step 6350/10000; xent: 2.75; lr: 0.0000251;  49 docs/s;   2679 sec\n",
      "[2019-10-15 18:15:55,784 INFO] Step 6400/10000; xent: 2.85; lr: 0.0000250;  47 docs/s;   2701 sec\n",
      "[2019-10-15 18:16:16,641 INFO] Step 6450/10000; xent: 2.78; lr: 0.0000249;  48 docs/s;   2722 sec\n",
      "[2019-10-15 18:16:37,458 INFO] Step 6500/10000; xent: 2.78; lr: 0.0000248;  48 docs/s;   2743 sec\n",
      "[2019-10-15 18:16:37,462 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_6500.pt\n",
      "[2019-10-15 18:16:59,595 INFO] Step 6550/10000; xent: 2.73; lr: 0.0000247;  46 docs/s;   2765 sec\n",
      "[2019-10-15 18:17:20,994 INFO] Step 6600/10000; xent: 2.76; lr: 0.0000246;  47 docs/s;   2786 sec\n",
      "[2019-10-15 18:17:41,749 INFO] Step 6650/10000; xent: 2.84; lr: 0.0000245;  49 docs/s;   2807 sec\n",
      "[2019-10-15 18:18:02,657 INFO] Step 6700/10000; xent: 2.85; lr: 0.0000244;  49 docs/s;   2828 sec\n",
      "[2019-10-15 18:18:23,380 INFO] Step 6750/10000; xent: 2.80; lr: 0.0000243;  49 docs/s;   2849 sec\n",
      "[2019-10-15 18:18:44,867 INFO] Step 6800/10000; xent: 2.77; lr: 0.0000243;  47 docs/s;   2870 sec\n",
      "[2019-10-15 18:19:05,681 INFO] Step 6850/10000; xent: 2.81; lr: 0.0000242;  49 docs/s;   2891 sec\n",
      "[2019-10-15 18:19:26,491 INFO] Step 6900/10000; xent: 2.84; lr: 0.0000241;  48 docs/s;   2912 sec\n",
      "[2019-10-15 18:19:47,512 INFO] Step 6950/10000; xent: 2.84; lr: 0.0000240;  48 docs/s;   2933 sec\n",
      "[2019-10-15 18:20:09,154 INFO] Step 7000/10000; xent: 2.79; lr: 0.0000239;  47 docs/s;   2954 sec\n",
      "[2019-10-15 18:20:09,158 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_7000.pt\n",
      "[2019-10-15 18:20:31,532 INFO] Step 7050/10000; xent: 2.89; lr: 0.0000238;  45 docs/s;   2977 sec\n",
      "[2019-10-15 18:20:52,351 INFO] Step 7100/10000; xent: 2.83; lr: 0.0000237;  49 docs/s;   2998 sec\n",
      "[2019-10-15 18:21:13,252 INFO] Step 7150/10000; xent: 2.85; lr: 0.0000237;  48 docs/s;   3018 sec\n",
      "[2019-10-15 18:21:34,970 INFO] Step 7200/10000; xent: 2.85; lr: 0.0000236;  46 docs/s;   3040 sec\n",
      "[2019-10-15 18:21:55,777 INFO] Step 7250/10000; xent: 2.79; lr: 0.0000235;  48 docs/s;   3061 sec\n",
      "[2019-10-15 18:22:16,639 INFO] Step 7300/10000; xent: 2.82; lr: 0.0000234;  48 docs/s;   3082 sec\n",
      "[2019-10-15 18:22:37,482 INFO] Step 7350/10000; xent: 2.80; lr: 0.0000233;  48 docs/s;   3103 sec\n",
      "[2019-10-15 18:22:58,914 INFO] Step 7400/10000; xent: 2.79; lr: 0.0000232;  47 docs/s;   3124 sec\n",
      "[2019-10-15 18:23:19,645 INFO] Step 7450/10000; xent: 2.80; lr: 0.0000232;  49 docs/s;   3145 sec\n",
      "[2019-10-15 18:23:40,337 INFO] Step 7500/10000; xent: 2.77; lr: 0.0000231;  48 docs/s;   3166 sec\n",
      "[2019-10-15 18:23:40,340 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_7500.pt\n",
      "[2019-10-15 18:24:02,476 INFO] Step 7550/10000; xent: 2.71; lr: 0.0000230;  46 docs/s;   3188 sec\n",
      "[2019-10-15 18:24:23,953 INFO] Step 7600/10000; xent: 2.69; lr: 0.0000229;  47 docs/s;   3209 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 18:24:44,627 INFO] Step 7650/10000; xent: 2.76; lr: 0.0000229;  49 docs/s;   3230 sec\n",
      "[2019-10-15 18:25:05,523 INFO] Step 7700/10000; xent: 2.82; lr: 0.0000228;  48 docs/s;   3251 sec\n",
      "[2019-10-15 18:25:27,089 INFO] Step 7750/10000; xent: 2.86; lr: 0.0000227;  47 docs/s;   3272 sec\n",
      "[2019-10-15 18:25:47,876 INFO] Step 7800/10000; xent: 2.80; lr: 0.0000226;  48 docs/s;   3293 sec\n",
      "[2019-10-15 18:26:08,708 INFO] Step 7850/10000; xent: 2.83; lr: 0.0000226;  48 docs/s;   3314 sec\n",
      "[2019-10-15 18:26:29,493 INFO] Step 7900/10000; xent: 2.83; lr: 0.0000225;  49 docs/s;   3335 sec\n",
      "[2019-10-15 18:26:51,045 INFO] Step 7950/10000; xent: 2.76; lr: 0.0000224;  47 docs/s;   3356 sec\n",
      "[2019-10-15 18:27:11,842 INFO] Step 8000/10000; xent: 2.78; lr: 0.0000224;  48 docs/s;   3377 sec\n",
      "[2019-10-15 18:27:11,846 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_8000.pt\n",
      "[2019-10-15 18:27:33,976 INFO] Step 8050/10000; xent: 2.82; lr: 0.0000223;  45 docs/s;   3399 sec\n",
      "[2019-10-15 18:27:54,858 INFO] Step 8100/10000; xent: 2.75; lr: 0.0000222;  49 docs/s;   3420 sec\n",
      "[2019-10-15 18:28:16,407 INFO] Step 8150/10000; xent: 2.84; lr: 0.0000222;  47 docs/s;   3442 sec\n",
      "[2019-10-15 18:28:37,145 INFO] Step 8200/10000; xent: 2.86; lr: 0.0000221;  49 docs/s;   3462 sec\n",
      "[2019-10-15 18:28:57,996 INFO] Step 8250/10000; xent: 2.72; lr: 0.0000220;  48 docs/s;   3483 sec\n",
      "[2019-10-15 18:29:18,776 INFO] Step 8300/10000; xent: 2.77; lr: 0.0000220;  48 docs/s;   3504 sec\n",
      "[2019-10-15 18:29:40,224 INFO] Step 8350/10000; xent: 2.67; lr: 0.0000219;  47 docs/s;   3525 sec\n",
      "[2019-10-15 18:30:01,190 INFO] Step 8400/10000; xent: 2.80; lr: 0.0000218;  49 docs/s;   3546 sec\n",
      "[2019-10-15 18:30:21,820 INFO] Step 8450/10000; xent: 2.81; lr: 0.0000218;  49 docs/s;   3567 sec\n",
      "[2019-10-15 18:30:42,632 INFO] Step 8500/10000; xent: 2.80; lr: 0.0000217;  48 docs/s;   3588 sec\n",
      "[2019-10-15 18:30:42,636 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_8500.pt\n",
      "[2019-10-15 18:31:05,497 INFO] Step 8550/10000; xent: 2.80; lr: 0.0000216;  44 docs/s;   3611 sec\n",
      "[2019-10-15 18:31:26,353 INFO] Step 8600/10000; xent: 2.77; lr: 0.0000216;  48 docs/s;   3632 sec\n",
      "[2019-10-15 18:31:47,131 INFO] Step 8650/10000; xent: 2.82; lr: 0.0000215;  49 docs/s;   3652 sec\n",
      "[2019-10-15 18:32:08,004 INFO] Step 8700/10000; xent: 2.85; lr: 0.0000214;  48 docs/s;   3673 sec\n",
      "[2019-10-15 18:32:29,641 INFO] Step 8750/10000; xent: 2.82; lr: 0.0000214;  47 docs/s;   3695 sec\n",
      "[2019-10-15 18:32:50,512 INFO] Step 8800/10000; xent: 2.80; lr: 0.0000213;  48 docs/s;   3716 sec\n",
      "[2019-10-15 18:33:11,332 INFO] Step 8850/10000; xent: 2.73; lr: 0.0000213;  48 docs/s;   3737 sec\n",
      "[2019-10-15 18:33:32,160 INFO] Step 8900/10000; xent: 2.78; lr: 0.0000212;  48 docs/s;   3757 sec\n",
      "[2019-10-15 18:33:53,602 INFO] Step 8950/10000; xent: 2.79; lr: 0.0000211;  47 docs/s;   3779 sec\n",
      "[2019-10-15 18:34:14,359 INFO] Step 9000/10000; xent: 2.79; lr: 0.0000211;  48 docs/s;   3800 sec\n",
      "[2019-10-15 18:34:14,363 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_9000.pt\n",
      "[2019-10-15 18:34:36,486 INFO] Step 9050/10000; xent: 2.75; lr: 0.0000210;  45 docs/s;   3822 sec\n",
      "[2019-10-15 18:34:57,361 INFO] Step 9100/10000; xent: 2.70; lr: 0.0000210;  48 docs/s;   3843 sec\n",
      "[2019-10-15 18:35:18,931 INFO] Step 9150/10000; xent: 2.80; lr: 0.0000209;  47 docs/s;   3864 sec\n",
      "[2019-10-15 18:35:39,714 INFO] Step 9200/10000; xent: 2.86; lr: 0.0000209;  49 docs/s;   3885 sec\n",
      "[2019-10-15 18:36:00,636 INFO] Step 9250/10000; xent: 2.80; lr: 0.0000208;  48 docs/s;   3906 sec\n",
      "[2019-10-15 18:36:21,489 INFO] Step 9300/10000; xent: 2.79; lr: 0.0000207;  49 docs/s;   3927 sec\n",
      "[2019-10-15 18:36:42,979 INFO] Step 9350/10000; xent: 2.86; lr: 0.0000207;  47 docs/s;   3948 sec\n",
      "[2019-10-15 18:37:03,791 INFO] Step 9400/10000; xent: 2.80; lr: 0.0000206;  48 docs/s;   3969 sec\n",
      "[2019-10-15 18:37:24,571 INFO] Step 9450/10000; xent: 2.73; lr: 0.0000206;  49 docs/s;   3990 sec\n",
      "[2019-10-15 18:37:45,326 INFO] Step 9500/10000; xent: 2.84; lr: 0.0000205;  48 docs/s;   4010 sec\n",
      "[2019-10-15 18:37:45,330 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_9500.pt\n",
      "[2019-10-15 18:38:08,399 INFO] Step 9550/10000; xent: 2.79; lr: 0.0000205;  44 docs/s;   4034 sec\n",
      "[2019-10-15 18:38:29,389 INFO] Step 9600/10000; xent: 2.85; lr: 0.0000204;  49 docs/s;   4055 sec\n",
      "[2019-10-15 18:38:50,108 INFO] Step 9650/10000; xent: 2.76; lr: 0.0000204;  48 docs/s;   4075 sec\n",
      "[2019-10-15 18:39:10,997 INFO] Step 9700/10000; xent: 2.84; lr: 0.0000203;  48 docs/s;   4096 sec\n",
      "[2019-10-15 18:39:32,498 INFO] Step 9750/10000; xent: 2.78; lr: 0.0000203;  47 docs/s;   4118 sec\n",
      "[2019-10-15 18:39:53,372 INFO] Step 9800/10000; xent: 2.84; lr: 0.0000202;  48 docs/s;   4139 sec\n",
      "[2019-10-15 18:40:14,324 INFO] Step 9850/10000; xent: 2.85; lr: 0.0000202;  48 docs/s;   4159 sec\n",
      "[2019-10-15 18:40:35,114 INFO] Step 9900/10000; xent: 2.75; lr: 0.0000201;  48 docs/s;   4180 sec\n",
      "[2019-10-15 18:40:56,505 INFO] Step 9950/10000; xent: 2.78; lr: 0.0000201;  47 docs/s;   4202 sec\n",
      "[2019-10-15 18:41:17,372 INFO] Step 10000/10000; xent: 2.85; lr: 0.0000200;  48 docs/s;   4223 sec\n",
      "[2019-10-15 18:41:17,376 INFO] Saving checkpoint ./models/transformer0.37907517824181713/model_step_10000.pt\n"
     ]
    }
   ],
   "source": [
    "bertsum_model.fit(device_id, training_data_files, train_steps=train_steps, train_from=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from bertsum.models.data_loader  import DataIterator,Batch,Dataloader\n",
    "import os\n",
    "if USE_PREPROCESSED_DATA is False: \n",
    "    test_dataset=torch.load(PROCESSED_TEST_FILE)\n",
    "else:\n",
    "    test_dataset=[]\n",
    "    for i in range(0,6):\n",
    "        filename = os.path.join(BERT_DATA_PATH, \"cnndm.test.{0}.bert.pt\".format(i))\n",
    "        test_dataset.extend(torch.load(filename))\n",
    "\n",
    "    \n",
    "def get_data_iter(dataset,is_test=False, batch_size=3000):\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    test_data_iter = None\n",
    "    test_data_iter  = DataIterator(args, dataset, args.batch_size, 'cuda', is_test=is_test, shuffle=False, sort=False)\n",
    "    return test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 18:42:17,589 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 0\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "checkpoint_to_test = 10000\n",
    "model_for_test =  os.path.join(model_base_path + encoder + str(random_number), f\"model_step_{checkpoint_to_test}.pt\")\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n",
    "                                   test_from=model_for_test,\n",
    "                                   sentence_seperator='<q>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-15 18:45:18,185 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-15 18:45:18,185 INFO] Writing summaries.\n",
      "2019-10-15 18:45:18,194 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmpsubsloro/system and model files to ./results/tmpsubsloro/model.\n",
      "[2019-10-15 18:45:18,194 INFO] Processing summaries. Saving system files to ./results/tmpsubsloro/system and model files to ./results/tmpsubsloro/model.\n",
      "2019-10-15 18:45:18,195 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-15-18-45-16/candidate/.\n",
      "[2019-10-15 18:45:18,195 INFO] Processing files in ./results/rouge-tmp-2019-10-15-18-45-16/candidate/.\n",
      "2019-10-15 18:45:19,514 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpsubsloro/system.\n",
      "[2019-10-15 18:45:19,514 INFO] Saved processed files to ./results/tmpsubsloro/system.\n",
      "2019-10-15 18:45:19,516 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-15-18-45-16/reference/.\n",
      "[2019-10-15 18:45:19,516 INFO] Processing files in ./results/rouge-tmp-2019-10-15-18-45-16/reference/.\n",
      "2019-10-15 18:45:20,654 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpsubsloro/model.\n",
      "[2019-10-15 18:45:20,654 INFO] Saved processed files to ./results/tmpsubsloro/model.\n",
      "2019-10-15 18:45:20,735 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmp4yrd5g_s/rouge_conf.xml\n",
      "[2019-10-15 18:45:20,735 INFO] Written ROUGE configuration to ./results/tmp4yrd5g_s/rouge_conf.xml\n",
      "2019-10-15 18:45:20,736 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp4yrd5g_s/rouge_conf.xml\n",
      "[2019-10-15 18:45:20,736 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp4yrd5g_s/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53085 (95%-conf.int. 0.52800 - 0.53379)\n",
      "1 ROUGE-1 Average_P: 0.37857 (95%-conf.int. 0.37621 - 0.38098)\n",
      "1 ROUGE-1 Average_F: 0.42727 (95%-conf.int. 0.42510 - 0.42940)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24509 (95%-conf.int. 0.24219 - 0.24801)\n",
      "1 ROUGE-2 Average_P: 0.17560 (95%-conf.int. 0.17344 - 0.17784)\n",
      "1 ROUGE-2 Average_F: 0.19750 (95%-conf.int. 0.19532 - 0.19981)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.48578 (95%-conf.int. 0.48310 - 0.48855)\n",
      "1 ROUGE-L Average_P: 0.34703 (95%-conf.int. 0.34466 - 0.34939)\n",
      "1 ROUGE-L Average_F: 0.39138 (95%-conf.int. 0.38922 - 0.39357)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "rouge_baseline = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .<q>he was flown back to chicago via air ambulance on march 20 , but he died on sunday .<q>he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "args=Bunch({\"max_nsents\": int(1e5), \n",
    "            \"max_src_ntokens\": int(2e6), \n",
    "            \"min_nsents\": -1, \n",
    "            \"min_src_ntokens\": -1,  \n",
    "            \"use_interval\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 18:47:09,248 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/daden/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from bertsum.prepro.data_builder import BertData\n",
    "bertdata = BertData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from utils_nlp.dataset.harvardnlp_cnndm import preprocess\n",
    "from nltk import tokenize\n",
    "from bertsum.others.utils import clean\n",
    "def preprocess_source(line):\n",
    "    return preprocess((line, [clean, tokenize.sent_tokenize], nltk.word_tokenize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n'.join(test_dataset[0]['src_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_src = preprocess_source(text)\n",
    "b_data = bertdata.preprocess(new_src, None, None)\n",
    "indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'university',\n",
       "  'of',\n",
       "  'iowa',\n",
       "  'student',\n",
       "  'has',\n",
       "  'died',\n",
       "  'nearly',\n",
       "  'three',\n",
       "  'months',\n",
       "  'after',\n",
       "  'a',\n",
       "  'fall',\n",
       "  'in',\n",
       "  'rome',\n",
       "  'in',\n",
       "  'a',\n",
       "  'suspected',\n",
       "  'robbery',\n",
       "  'attack',\n",
       "  'in',\n",
       "  'rome',\n",
       "  '.'],\n",
       " ['andrew',\n",
       "  'mogni',\n",
       "  ',',\n",
       "  '20',\n",
       "  ',',\n",
       "  'from',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  ',',\n",
       "  'illinois',\n",
       "  ',',\n",
       "  'had',\n",
       "  'only',\n",
       "  'just',\n",
       "  'arrived',\n",
       "  'for',\n",
       "  'a',\n",
       "  'semester',\n",
       "  'program',\n",
       "  'in',\n",
       "  'italy',\n",
       "  'when',\n",
       "  'the',\n",
       "  'incident',\n",
       "  'happened',\n",
       "  'in',\n",
       "  'january',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'flown',\n",
       "  'back',\n",
       "  'to',\n",
       "  'chicago',\n",
       "  'via',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'on',\n",
       "  'march',\n",
       "  '20',\n",
       "  ',',\n",
       "  'but',\n",
       "  'he',\n",
       "  'died',\n",
       "  'on',\n",
       "  'sunday',\n",
       "  '.'],\n",
       " ['andrew',\n",
       "  'mogni',\n",
       "  ',',\n",
       "  '20',\n",
       "  ',',\n",
       "  'from',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  ',',\n",
       "  'illinois',\n",
       "  ',',\n",
       "  'a',\n",
       "  'university',\n",
       "  'of',\n",
       "  'iowa',\n",
       "  'student',\n",
       "  'has',\n",
       "  'died',\n",
       "  'nearly',\n",
       "  'three',\n",
       "  'months',\n",
       "  'after',\n",
       "  'a',\n",
       "  'fall',\n",
       "  'in',\n",
       "  'rome',\n",
       "  'in',\n",
       "  'a',\n",
       "  'suspected',\n",
       "  'robbery',\n",
       "  'he',\n",
       "  'was',\n",
       "  'taken',\n",
       "  'to',\n",
       "  'a',\n",
       "  'medical',\n",
       "  'facility',\n",
       "  'in',\n",
       "  'the',\n",
       "  'chicago',\n",
       "  'area',\n",
       "  ',',\n",
       "  'close',\n",
       "  'to',\n",
       "  'his',\n",
       "  'family',\n",
       "  'home',\n",
       "  'in',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'died',\n",
       "  'on',\n",
       "  'sunday',\n",
       "  'at',\n",
       "  'northwestern',\n",
       "  'memorial',\n",
       "  'hospital',\n",
       "  '-',\n",
       "  'medical',\n",
       "  'examiner',\n",
       "  \"'s\",\n",
       "  'office',\n",
       "  'spokesman',\n",
       "  'frank',\n",
       "  'shuftan',\n",
       "  'says',\n",
       "  'a',\n",
       "  'cause',\n",
       "  'of',\n",
       "  'death',\n",
       "  'wo',\n",
       "  \"n't\",\n",
       "  'be',\n",
       "  'released',\n",
       "  'until',\n",
       "  'monday',\n",
       "  'at',\n",
       "  'the',\n",
       "  'earliest',\n",
       "  '.'],\n",
       " ['initial',\n",
       "  'police',\n",
       "  'reports',\n",
       "  'indicated',\n",
       "  'the',\n",
       "  'fall',\n",
       "  'was',\n",
       "  'an',\n",
       "  'accident',\n",
       "  'but',\n",
       "  'authorities',\n",
       "  'are',\n",
       "  'investigating',\n",
       "  'the',\n",
       "  'possibility',\n",
       "  'that',\n",
       "  'mogni',\n",
       "  'was',\n",
       "  'robbed',\n",
       "  '.'],\n",
       " ['on',\n",
       "  'sunday',\n",
       "  ',',\n",
       "  'his',\n",
       "  'cousin',\n",
       "  'abby',\n",
       "  'wrote',\n",
       "  'online',\n",
       "  ':',\n",
       "  '`',\n",
       "  'this',\n",
       "  'morning',\n",
       "  'my',\n",
       "  'cousin',\n",
       "  'andrew',\n",
       "  \"'s\",\n",
       "  'soul',\n",
       "  'was',\n",
       "  'lifted',\n",
       "  'up',\n",
       "  'to',\n",
       "  'heaven',\n",
       "  '.'],\n",
       " ['initial',\n",
       "  'police',\n",
       "  'reports',\n",
       "  'indicated',\n",
       "  'the',\n",
       "  'fall',\n",
       "  'was',\n",
       "  'an',\n",
       "  'accident',\n",
       "  'but',\n",
       "  'authorities',\n",
       "  'are',\n",
       "  'investigating',\n",
       "  'the',\n",
       "  'possibility',\n",
       "  'that',\n",
       "  'mogni',\n",
       "  'was',\n",
       "  'robbed',\n",
       "  '`',\n",
       "  'at',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'january',\n",
       "  'he',\n",
       "  'went',\n",
       "  'to',\n",
       "  'rome',\n",
       "  'to',\n",
       "  'study',\n",
       "  'aboard',\n",
       "  'and',\n",
       "  'on',\n",
       "  'the',\n",
       "  'way',\n",
       "  'home',\n",
       "  'from',\n",
       "  'a',\n",
       "  'party',\n",
       "  'he',\n",
       "  'was',\n",
       "  'brutally',\n",
       "  'attacked',\n",
       "  'and',\n",
       "  'thrown',\n",
       "  'off',\n",
       "  'a',\n",
       "  '40ft',\n",
       "  'bridge',\n",
       "  'and',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'concrete',\n",
       "  'below',\n",
       "  '.'],\n",
       " ['`',\n",
       "  'he',\n",
       "  'was',\n",
       "  'in',\n",
       "  'a',\n",
       "  'coma',\n",
       "  'and',\n",
       "  'in',\n",
       "  'critical',\n",
       "  'condition',\n",
       "  'for',\n",
       "  'months',\n",
       "  '.',\n",
       "  \"'\"],\n",
       " ['paula',\n",
       "  'barnett',\n",
       "  ',',\n",
       "  'who',\n",
       "  'said',\n",
       "  'she',\n",
       "  'is',\n",
       "  'a',\n",
       "  'close',\n",
       "  'family',\n",
       "  'friend',\n",
       "  ',',\n",
       "  'told',\n",
       "  'my',\n",
       "  'suburban',\n",
       "  'life',\n",
       "  ',',\n",
       "  'that',\n",
       "  'mogni',\n",
       "  'had',\n",
       "  'only',\n",
       "  'been',\n",
       "  'in',\n",
       "  'the',\n",
       "  'country',\n",
       "  'for',\n",
       "  'six',\n",
       "  'hours',\n",
       "  'when',\n",
       "  'the',\n",
       "  'incident',\n",
       "  'happened',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'said',\n",
       "  'he',\n",
       "  'was',\n",
       "  'was',\n",
       "  'alone',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'of',\n",
       "  'the',\n",
       "  'alleged',\n",
       "  'assault',\n",
       "  'and',\n",
       "  'personal',\n",
       "  'items',\n",
       "  'were',\n",
       "  'stolen',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'added',\n",
       "  'that',\n",
       "  'he',\n",
       "  'was',\n",
       "  'in',\n",
       "  'a',\n",
       "  'non-medically',\n",
       "  'induced',\n",
       "  'coma',\n",
       "  ',',\n",
       "  'having',\n",
       "  'suffered',\n",
       "  'serious',\n",
       "  'infection',\n",
       "  'and',\n",
       "  'internal',\n",
       "  'bleeding',\n",
       "  '.'],\n",
       " ['mogni',\n",
       "  'was',\n",
       "  'a',\n",
       "  'third-year',\n",
       "  'finance',\n",
       "  'major',\n",
       "  'from',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  ',',\n",
       "  'ill.',\n",
       "  ',',\n",
       "  'who',\n",
       "  'was',\n",
       "  'participating',\n",
       "  'in',\n",
       "  'a',\n",
       "  'semester-long',\n",
       "  'program',\n",
       "  'at',\n",
       "  'john',\n",
       "  'cabot',\n",
       "  'university',\n",
       "  '.'],\n",
       " ['mogni',\n",
       "  'belonged',\n",
       "  'to',\n",
       "  'the',\n",
       "  'school',\n",
       "  \"'s\",\n",
       "  'chapter',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sigma',\n",
       "  'nu',\n",
       "  'fraternity',\n",
       "  ',',\n",
       "  'reports',\n",
       "  'the',\n",
       "  'chicago',\n",
       "  'tribune',\n",
       "  'who',\n",
       "  'posted',\n",
       "  'a',\n",
       "  'sign',\n",
       "  'outside',\n",
       "  'a',\n",
       "  'building',\n",
       "  'reading',\n",
       "  '`',\n",
       "  'pray',\n",
       "  'for',\n",
       "  'mogni',\n",
       "  '.',\n",
       "  \"'\"],\n",
       " ['the',\n",
       "  'fraternity',\n",
       "  \"'s\",\n",
       "  'iowa',\n",
       "  'chapter',\n",
       "  'announced',\n",
       "  'sunday',\n",
       "  'afternoon',\n",
       "  'via',\n",
       "  'twitter',\n",
       "  'that',\n",
       "  'a',\n",
       "  'memorial',\n",
       "  'service',\n",
       "  'will',\n",
       "  'be',\n",
       "  'held',\n",
       "  'on',\n",
       "  'campus',\n",
       "  'to',\n",
       "  'remember',\n",
       "  'mogni',\n",
       "  '.']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed ` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " \"` he was in a coma and in critical condition for months . '\",\n",
       " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data_dict['src_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data_dict['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 18:47:11,792 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 0\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "model_for_test =  os.path.join(model_base_path + encoder + str(random_number), f\"model_step_{checkpoint_to_test}.pt\")\n",
    "#get_data_iter(output,batch_size=30000)\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter([b_data_dict], False),\n",
    "                                   test_from=model_for_test, sentence_seperator='<q>' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .<q>he was flown back to chicago via air ambulance on march 20 , but he died on sunday .<q>a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
