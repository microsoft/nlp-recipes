{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GenSen Model with Distributed Pytorch on AML\n",
    "In this tutorial, you will train a GenSen model with PyTorch on AML using distributed training across a GPU cluster. This could also be a generic guideline to train models using GPU cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Go through the [Configuration](../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`\n",
    "* Review the [tutorial](../train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb) on single-node PyTorch training using Azure Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.23\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n"
     ]
    }
   ],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\lishao\\Project\\NLP\\examples\\02-model\\gensen\\.azureml\\config.json\n",
      "Workspace name: MAIDAPNLP\n",
      "Azure region: eastus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource. Specifically, the below code creates an `STANDARD_NC6` GPU cluster that autoscales from `0` to `4` nodes.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "**Use Standard_NC6 for now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-04-19T15:48:17.957000+00:00', 'errors': None, 'creationTime': '2019-04-17T17:21:26.968570+00:00', 'modifiedTime': '2019-04-17T17:27:28.740980+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT7200S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on.\n",
    "\n",
    "`project_folder` contains all the code you want to submit to AmlCompute to run. The size of the folder can not exceed 300Mb. In GenSen model, it loads large pre-trained embedding files to the model. Thus, we need to save large files in datastore and only uploads code to `project_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the path to where your model code locates.\n",
    "# To download some of the data required to train a GenSen model in the original paper, run:\n",
    "# https://github.com/Maluuba/gensen/blob/master/get_data.sh.\n",
    "project_folder = '../../../models/advanced/gensen/amlcode/'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to Datastore\n",
    "To download some of the data required to train a GenSen model, run the bash file [here](https://github.com/Maluuba/gensen/blob/master/get_data.sh). Make sure to upload all the large files to azure file share. You can access to datastore by using `ds.as_mount()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "ds = Datastore.register_azure_file_share(workspace=ws,\n",
    "                                        datastore_name= 'GenSen',\n",
    "                                        file_share_name='azureml-filestore-09b72610-7938-4ed2-86a2-5004896b12d9',\n",
    "                                        account_name='maidapnlp0056795534',\n",
    "                                        account_key='8LtGFZErNlvI6fSrgODqCxJCckkVgq3AL/5S/8ma7Re7xUHgWrNRCfTFnP/QDhF7KDY6ScAORsUpSm7ziog5/Q==')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the contents of the data directory to the path ./data on the default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files from local.\n",
    "ds.upload(src_dir='data', target_path='data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_gensen"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.as_mount()\n",
    "# We can fetch the datastore path by: os.environ['AZUREML_DATAREFERENCE_datarefname'].\n",
    "# path_on_datastore = 'data/'\n",
    "# ds_data = ds.path(path_on_datastore)\n",
    "# print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that we have the AmlCompute ready to go, let's run our distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the script for distributed training of GENSEN is already provided for you at `train.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [metric logging](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#logging) capabilities, you will have to add a small amount of Azure ML logic inside your training script. In this example, at each logging interval, we will log the loss for that minibatch to our Azure ML run.\n",
    "\n",
    "To do so, in `train.py`, we will first access the Azure ML `Run` object within the script:\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "Later within the script, we log the loss metric to our run:\n",
    "```Python\n",
    "run.log('loss', loss.item())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed PyTorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, get_run\n",
    "\n",
    "experiment_name = 'pytorch-gensen'\n",
    "experiment = Experiment(ws, name=experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch estimator\n",
    "The Azure ML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--config': 'example_config.json',\n",
    "    '--data_folder': ds.as_mount()}\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder,\n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='train.py',\n",
    "                    node_count=4,\n",
    "                    process_count_per_node=1,\n",
    "                    distributed_backend='mpi',\n",
    "                    use_gpu=True,\n",
    "                    conda_packages=['scikit-learn=0.20.3']\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code specifies that we will run our training script on `4` nodes, with one worker per node. In order to execute a distributed run using GPU, you must provide the argument `use_gpu=True`. To execute a distributed run using MPI/Horovod, you must provide the argument `distributed_backend='mpi'`. Using this estimator with these settings, PyTorch, Horovod and their dependencies will be installed for you. If you are the first time to create a experiment, it may take longer to set up conda environments under `.azureml/conda_dependencies.yml`. After the first run, it will use the existing conda environments and directly run the code. However, if your script also uses other packages, make sure to install them via the `PyTorch` constructor's `pip_packages` or `conda_packages` parameters. The more required packages are stored in `.azureml/conda_dependencies.yml` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your estimator object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: pytorch-gensen,\n",
      "Id: pytorch-gensen_1555961816_f78c37cf,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Queued)\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel the job.\n",
    "It's better to cancel the job manually to make sure you does not waste resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel the job with id.\n",
    "# job_id = \"pytorch-gensen_1555533596_d9cc75fe\"\n",
    "# run = get_run(experiment, job_id)\n",
    "\n",
    "# Cancel jobs.\n",
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes. You can see that the widget automatically plots and visualizes the loss metric that we logged to the Azure ML run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb062e94d1d4a96a0ce148fc5b89260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 'sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: pytorch-gensen_1555961510_2ad76014\n",
      "\n",
      "Streaming azureml-logs/60_control_log_rank_0.txt\n",
      "================================================\n",
      "\n",
      "This is an MPI job. Rank:0\n",
      "Streaming log file azureml-logs/60_control_log_rank_0.txt\n",
      "Streaming log file azureml-logs/80_driver_log_rank_0.txt\n",
      "\n",
      "Streaming azureml-logs/80_driver_log_rank_1.txt\n",
      "===============================================\n",
      "\n",
      "\n",
      "\n",
      "The experiment failed. Finalizing run...\n",
      "Traceback (most recent call last):\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 161, in <module>\n",
      "    execute_with_context(cm_objects, options.invocation)\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 90, in execute_with_context\n",
      "    runpy.run_path(sys.argv[0], globals(), run_name=\"__main__\")\n",
      "  File \"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"train.py\", line 60, in <module>\n",
      "    config = read_config(config_file_path)\n",
      "  File \"train.py\", line 45, in read_config\n",
      "    json_object = json.load(open(json_file, 'r', encoding='utf-8'))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'example_config.json'\n",
      "\n",
      "Streaming azureml-logs/80_driver_log_rank_0.txt\n",
      "===============================================\n",
      "\n",
      "\n",
      "\n",
      "The experiment failed. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "3 items cleaning up...\n",
      "Cleanup took 0.7541937828063965 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 161, in <module>\n",
      "    execute_with_context(cm_objects, options.invocation)\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 90, in execute_with_context\n",
      "    runpy.run_path(sys.argv[0], globals(), run_name=\"__main__\")\n",
      "  File \"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"train.py\", line 60, in <module>\n",
      "    config = read_config(config_file_path)\n",
      "  File \"train.py\", line 45, in read_config\n",
      "    json_object = json.load(open(json_file, 'r', encoding='utf-8'))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'example_config.json'\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: pytorch-gensen_1555961510_2ad76014\n",
      "\n",
      "Warnings:\n",
      "Ignoring the provided userId for the createdBy property.\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"UserError\",\n",
      "    \"message\": \"AmlCompute job failed.\\nJobFailed: Job failed with non-zero exit Code\",\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": null,\n",
      "    \"debugInfo\": null\n",
      "  },\n",
      "  \"correlation\": {\n",
      "    \"operation\": null,\n",
      "    \"request\": \"hnzgHhnMOEw=\"\n",
      "  },\n",
      "  \"environment\": \"eastus2\",\n",
      "  \"location\": \"eastus2\",\n",
      "  \"time\": \"2019-04-22T19:35:52.9172439+00:00\"\n",
      "}\n",
      "Failed to upload Batch AI logs to RunHistory. The logs can be accessed directly on the file share .\n",
      "\n",
      "\n",
      "Error:\n",
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"UserError\",\n",
      "        \"message\": \"[Errno 2] No such file or directory: 'example_config.json'\",\n",
      "        \"details\": [],\n",
      "        \"debugInfo\": {\n",
      "            \"type\": \"FileNotFoundError\",\n",
      "            \"message\": \"[Errno 2] No such file or directory: 'example_config.json'\",\n",
      "            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 90, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"train.py\\\", line 60, in <module>\\n    config = read_config(config_file_path)\\n  File \\\"train.py\\\", line 45, in read_config\\n    json_object = json.load(open(json_file, 'r', encoding='utf-8'))\\n\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"[Errno 2] No such file or directory: 'example_config.json'\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"FileNotFoundError\",\n            \"message\": \"[Errno 2] No such file or directory: 'example_config.json'\",\n            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 90, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"train.py\\\", line 60, in <module>\\n    config = read_config(config_file_path)\\n  File \\\"train.py\\\", line 45, in read_config\\n    json_object = json.load(open(json_file, 'r', encoding='utf-8'))\\n\"\n        }\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-68234af5c1cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# this provides a verbose log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\onboarding_aml\\lib\\site-packages\\azureml\\core\\script_run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[1;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[0mfile_handle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[0mwait_post_processing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwait_post_processing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                     raise_on_error=raise_on_error)\n\u001b[0m\u001b[0;32m     80\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\onboarding_aml\\lib\\site-packages\\azureml\\core\\script_run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[1;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mActivityFailedException\u001b[0m: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"[Errno 2] No such file or directory: 'example_config.json'\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"FileNotFoundError\",\n            \"message\": \"[Errno 2] No such file or directory: 'example_config.json'\",\n            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 90, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_4737f522821717a6daa7464dfd956f84/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"train.py\\\", line 60, in <module>\\n    config = read_config(config_file_path)\\n  File \\\"train.py\\\", line 45, in read_config\\n    json_object = json.load(open(json_file, 'r', encoding='utf-8'))\\n\"\n        }\n    }\n}"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "minxia"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "msauthor": "minxia"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
