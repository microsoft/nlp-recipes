{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["Copyright (c) Microsoft Corporation. All rights reserved.\n","\n","Licensed under the MIT License.\n","\n","### Intel NLP-Architect ABSA on AzureML \n","\n","This notebook contains an end-to-end walkthrough of using Azure Machine Learning Service to train, finetune and test [Aspect Based Sentiment Analysis Models using Intel's NLP Architect](http://nlp_architect.nervanasys.com/absa.html)\n","\n","### Prerequisites\n","\n","* Understand the architecture and terms introduced by Azure Machine Learning (AML)\n","* Have working Jupyter Notebook Environment. You can:\n","    - Install Python environment locally, as described below in **Local Installation**\n","    - Use [Azure Notebooks](https://docs.microsoft.com/ru-ru/azure/notebooks/azure-notebooks-overview/?wt.mc_id=absa-notebook-abornst). In this case you should upload the `absa.ipynb` file to a new Azure Notebooks project, or just clone the [GitHub Repo](https://github.com/microsoft/ignite-learning-paths/tree/master/aiml/aiml40).\n","* Azure Machine Learning Workspace in your Azure Subscription\n","\n","#### Local Installation\n","\n","Install the Python SDK: make sure to install notebook, and contrib:\n","\n","```shell\n","conda create -n azureml -y Python=3.6\n","source activate azureml\n","pip install --upgrade azureml-sdk[notebooks,contrib] \n","conda install ipywidgets\n","jupyter nbextension install --py --user azureml.widgets\n","jupyter nbextension enable azureml.widgets --user --py\n","```\n","\n","You will need to restart jupyter after this Detailed instructions are [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python/?WT.mc_id=absa-notebook-abornst)\n","\n","If you need a free trial account to get started you can get one [here](https://azure.microsoft.com/en-us/offers/ms-azr-0044p/?WT.mc_id=absa-notebook-abornst)\n","\n","#### Creating Azure ML Workspace\n","\n","Azure ML Workspace can be created by using one of the following ways:\n","* Manually through [Azure Portal](http://portal.azure.com/?WT.mc_id=absa-notebook-abornst) - [here is the complete walkthrough](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace/?wt.mc_id=absa-notebook-abornst)\n","* Using [Azure CLI](https://docs.microsoft.com/ru-ru/cli/azure/?view=azure-cli-latest&wt.mc_id=absa-notebook-abornst), using the following commands:\n","\n","```shell\n","az extension add -n azure-cli-ml\n","az group create -n absa -l westus2\n","az ml workspace create -w absa_space -g absa\n","```"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Initialize workspace\n","\n","To access an Azure ML Workspace, you will need to import the AML library and the following information:\n","* A name for your workspace (in our example - `absa_space`)\n","* Your subscription id (can be obtained by running `az account list`)\n","* The resource group name (in our case `absa`)\n","\n","Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace/?WT.mc_id=absa-notebook-abornst) object from the existing workspace you created in the Prerequisites step or create a new one. "]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from azureml.core import Workspace\n","\n","#subscription_id = ''\n","#resource_group  = 'absa'\n","#workspace_name  = 'absa_space'\n","#ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n","#ws.write_config()\n","\n","try:\n","    ws = Workspace.from_config()\n","    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n","    print('Library configuration succeeded')\n","except:\n","    print('Workspace not found')"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Compute"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["There are two computer option run once(preview) and persistent compute for this demo we will use persistent compute to learn more about run once compute check out the [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute?WT.mc_id=absa-notebook-abornst)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from azureml.core.compute import ComputeTarget, AmlCompute\n","from azureml.core.compute_target import ComputeTargetException\n","\n","# Choose a name for your CPU cluster\n","cluster_name = \"absa-cluster\"\n","\n","# Verify that cluster does not exist already\n","try:\n","    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n","    print('Found existing cluster, use it.')\n","except ComputeTargetException:\n","    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n","                                                           vm_priority='lowpriority',\n","                                                           min_nodes=1,\n","                                                           max_nodes=4)\n","    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n","\n","cluster.wait_for_completion(show_output=True)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Upload Data\n","\n","The dataset we are using comes from the [womens ecommerce clothing reviews dataset](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/) and is in the open domain, this can be replaced with any csv file with rows of text as the absa model is unsupervised. \n","\n","The documentation for uploading data can be found [here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.azureblobdatastore/?WT.mc_id=absa-notebook-abornst) for now we will us the ds.upload command. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["!wget -O 'dataset/glove.840B.300d.zip' 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n","# save 'dataset/clothing_absa_train.csv'\n","# save 'dataset/clothing-absa-validation.json'\n","# save 'dataset/clothing_absa_train_small.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os                            \n","lib_root = os.path.dirname(os.path.abspath(\"__file__\"))\n","ds = ws.get_default_datastore()\n","ds.upload('./dataset', target_path='clothing_data', overwrite=True, show_progress=True)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["Now the the glove file is uploaded to our datastore we can remove it from our local directory."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["!rm 'dataset/glove.840B.300d.zip'"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Train File"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile train.py\n","import argparse\n","import json\n","import os \n","from pathlib import Path\n","from nltk import flatten\n","from azureml.core import Run\n","from sklearn.metrics import f1_score\n","from azureml.core.model import Model\n","\n","# Load NLP Architect\n","from nlp_architect.models.absa.train.train import TrainSentiment\n","from nlp_architect.models.absa.inference.inference import SentimentInference\n","\n","# Inputs\n","parser = argparse.ArgumentParser(description='ABSA Train')\n","parser.add_argument('--data_folder', type=str, dest='data_folder', help='data folder mounting point')\n","parser.add_argument('--asp_thresh', type=int, default=3)\n","parser.add_argument('--op_thresh', type=int, default=2)\n","parser.add_argument('--max_iter', type=int, default=3)\n","\n","args = parser.parse_args()\n","\n","# Download ABSA dependencies including spacy parser and glove embeddings \n","from spacy.cli.download import download as spacy_download\n","from nlp_architect.utils.io import uncompress_file\n","from nlp_architect.models.absa import TRAIN_OUT\n","\n","spacy_download('en')\n","GLOVE_ZIP = os.path.join(args.data_folder, \n","                                 'clothing_data/glove.840B.300d.zip')\n","EMBEDDING_PATH = TRAIN_OUT / 'word_emb_unzipped' / 'glove.840B.300d.txt'\n","\n","\n","uncompress_file(GLOVE_ZIP, Path(EMBEDDING_PATH).parent)\n","\n","clothing_train = os.path.join(args.data_folder, \n","                                 'clothing_data/clothing_absa_train_small.csv')\n","\n","os.makedirs('outputs', exist_ok=True)\n","\n","train = TrainSentiment(asp_thresh=args.asp_thresh,\n","                       op_thresh=args.op_thresh, \n","                       max_iter=args.max_iter)\n","\n","opinion_lex, aspect_lex = train.run(data=clothing_train,\n","                                    out_dir = './outputs')\n","\n","# Evaluation \n","# Although ABSA is an unsupervised method it can be metriced with a small sample of labeled data\n","def doc2IO(doc):\n","    \"\"\"\n","    Converts ABSA doc to IO span format for evaluation\n","    \"\"\"\n","    index = 0\n","    aspect_indexes = []\n","    doc_json = json.loads(doc.json())\n","    tokens = doc_json[\"_doc_text\"].split()\n","    io = [[t,'O'] for t in tokens]\n","    for t_index, token in enumerate(tokens):\n","        for s in doc_json[\"_sentences\"]:\n","            for ev in s[\"_events\"]:\n","                for e in ev:\n","                    if e[\"_type\"] == \"ASPECT\":\n","                        if e[\"_start\"] == index and all(aspect[0] != t_index for aspect in aspect_indexes):\n","                            io[t_index][1] = \"{}-{}\".format(e[\"_text\"], e[\"_polarity\"])\n","        index += len(token) + 1\n","    \n","    return io\n","\n","inference = SentimentInference('./outputs/train_out/generated_aspect_lex.csv', \n","                               './outputs/train_out/generated_opinion_lex_reranked.csv')\n","\n","clothing_val = os.path.join(args.data_folder, \n","                                 'clothing_data/clothing-absa-validation.json')\n","\n","with open(clothing_val) as json_file:\n","    val = json.load(json_file)\n","\n","predictions = []\n","for doc in val[\"data\"]:\n","    doc_raw = \" \".join([token[0] for token in doc])\n","    sentiment_doc = inference.run(doc=doc_raw)\n","    predictions.append(doc2IO(sentiment_doc))\n","    \n","y_pred = flatten(predictions)[1::2]\n","y_true = flatten(val['data'])[1::2]\n","\n","from sklearn.metrics import f1_score\n","\n","# Log metrics\n","run = Run.get_context()\n","run.log('Aspect Lexicon Size', len(aspect_lex))\n","run.log('Opinion Lexicon Size', len(opinion_lex))\n","run.log('f1_weighted', float(f1_score(y_true, y_pred, average='weighted')))"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Create An Experiment\n","\n","Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment/?WT.mc_id=absa-notebook-abornst) to track all the runs in your workspace for this distributed PyTorch tutorial. "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from azureml.core import Experiment\n","experiment_name = 'absa'\n","exp = Experiment(workspace=ws, name=experiment_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from azureml.train.estimator import Estimator\n","\n","script_params = {\n","    '--data_folder': ds,\n","}\n","\n","nlp_est = Estimator(source_directory='.',\n","                   script_params=script_params,\n","                   compute_target=cluster,\n","                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n","                   entry_script='train.py',\n","                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa',\n","                                 'spacy==2.1.8']\n",")"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["To create a run we just submit our expierment as follows."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["run = exp.submit(nlp_est)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["Note: If you accidently run the following cell more than once you can cancel a run with the run.cancel() command."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run.cancel()"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["You can load any previous run using its run id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["run.id"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["run = [r for r in exp.get_runs() if r.id == 'put_run_id_here'][0]"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["Let's visualize our run:"]},{"cell_type":"code","execution_count":25,"metadata":{"scrolled":true},"outputs":[],"source":["from azureml.widgets import RunDetails\n","\n","RunDetails(run).show()"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Fine-Tuning NLP Archictect  with AzureML HyperDrive\n","Although ABSA is an unsupervised method it's hyper parameters such as the aspect and opinion word thresholds can be fined tuned if provided with a small sample of labeled data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from azureml.train.hyperdrive import *\n","import math\n","\n","param_sampling = RandomParameterSampling({\n","         '--asp_thresh': choice(range(2,5)),\n","         '--op_thresh': choice(range(2,5)), \n","         '--max_iter': choice(range(2,5))\n","    })"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Early Termination Policy\n","First we will define an early terminination policy. [Median stopping](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.medianstoppingpolicy?WT.mc_id=absa-notebook-abornst) is an early termination policy based on running averages of primary metrics reported by the runs. This policy computes running averages across all training runs and terminates runs whose performance is worse than the median of the running averages. \n","\n","This policy takes the following configuration parameters:\n","\n","- evaluation_interval: the frequency for applying the policy (optional parameter).\n","- delay_evaluation: delays the first policy evaluation for a specified number of intervals (optional parameter).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=0)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["Refer [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#specify-early-termination-policy?WT.mc_id=absa-notebook-abornst) for more information on the Median stopping policy and other policies available.\n","\n","Now that we've defined our early termination policy we can define our Hyper Drive configuration to maximize our Model's weighted F1 score. Hyper Drive can optimize any metric can be optimized as long as it's logged by the training script. \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hd_config = HyperDriveConfig(estimator=nlp_est,\n","                            hyperparameter_sampling=param_sampling,\n","                            policy=early_termination_policy,\n","                            primary_metric_name='f1_weighted',\n","                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n","                            max_total_runs=16,\n","                            max_concurrent_runs=4)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["Finally, lauch the hyperparameter tuning job."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["experiment = Experiment(workspace=ws, name='absa_hyperdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyperdrive_run = experiment.submit(hd_config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyperdrive_run.id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyperdrive_run = [r for r in experiment.get_runs() if r.id == 'absa_hyperdrive_1571092544235933'][0]"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Monitor HyperDrive runs\n","We can monitor the progress of the runs with the following Jupyter widget. "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":["from azureml.widgets import RunDetails\n","\n","RunDetails(hyperdrive_run).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyperdrive_run.cancel()"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Find and register the best model\n","Once all the runs complete, we can find the run that produced the model with the highest evaluation (METRIC TBD)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_run = hyperdrive_run.get_best_run_by_primary_metric()\n","best_run_metrics = best_run.get_metrics()\n","print(best_run)\n","print('Best Run is:\\n  F1: {0:.5f}'.format(\n","        best_run_metrics['f1_weighted']\n","     ))"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Register Model Outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aspect_lex = run.register_model(model_name='c_aspect_lex', model_path='outputs/train_out/generated_aspect_lex.csv')\n","opinion_lex = run.register_model(model_name='c_opinion_lex', model_path='outputs/train_out/generated_opinion_lex_reranked.csv')"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Test Locally\n","\n","### Install Local PIP Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install git+https://github.com/NervanaSystems/nlp-architect.git@absa"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install spacy==2.0.18"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Load Model From AzureML"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["from azureml.core.model import Model\n","from nlp_architect.models.absa.inference.inference import SentimentInference\n","c_aspect_lex = Model._get_model_path_remote('c_aspect_lex', 1, ws)\n","c_opinion_lex = Model._get_model_path_remote('c_opinion_lex', 1, ws)   \n","inference = SentimentInference(c_aspect_lex, c_opinion_lex)\n"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Run Model On Sample Data "]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["docs = [\"Loved the sweater but hated the pants\",\n","       \"Really great outfit, but the shirt is the wrong size\",\n","       \"I absolutely love this jacket! i wear it almost everyday. works as a cardigan or a jacket. my favorite retailer purchase so far\"]\n","\n","sentiment_docs = []\n","\n","for doc_raw in docs:\n","    sentiment_doc = inference.run(doc=doc_raw)\n","    sentiment_docs.append(sentiment_doc)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Visualize Model Results"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["import spacy\n","from spacy import displacy\n","from nlp_architect.models.absa.inference.data_types import TermType\n","ents = []\n","for doc in sentiment_docs:    \n","    if doc:\n","        doc_viz = {'text':doc._doc_text, 'ents':[]}\n","        for s in doc._sentences:\n","            for ev in s._events:\n","                for e in ev:\n","                    if e._type == TermType.ASPECT:\n","                        ent = {'start': e._start, 'end': e._start + e._len,\n","                               'label':str(e._polarity.value), \n","                               'text':str(e._text)}\n","                        if all(kown_e['start'] != ent['start'] for kown_e in ents):\n","                            ents.append(ent)\n","                            doc_viz['ents'].append(ent)\n","        doc_viz['ents'].sort(key=lambda m: m[\"start\"])\n","        displacy.render(doc_viz, style=\"ent\", options={'colors':{'POS':'#7CFC00', 'NEG':'#FF0000'}}, manual=True)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Package Model For Deployment\n","\n","## Create scoring script\n","Create the scoring script, called score.py, used by the web service call to show how to use the model.\n","\n","You must include two required functions into the scoring script:\n","\n","The init() function, which typically loads the model into a global object. This function is run only once when the Docker container is started.\n","\n","The run(input_data) function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":["%%writefile score.py\n","from azureml.core.model import Model\n","from nlp_architect.models.absa.inference.inference import SentimentInference\n","from spacy.cli.download import download as spacy_download\n","\n","\n","def init():\n","    \"\"\"\n","    Set up the ABSA model for Inference  \n","    \"\"\"\n","    global SentInference\n","    spacy_download('en')\n","    aspect_lex = Model.get_model_path('c_aspect_lex')\n","    opinion_lex = Model.get_model_path('c_opinion_lex') \n","    SentInference = SentimentInference(aspect_lex, opinion_lex)\n","\n","def run(raw_data):\n","    \"\"\"\n","    Evaluate the model and return JSON string\n","    \"\"\"\n","    sentiment_doc = SentInference.run(doc=raw_data)\n","    return sentiment_doc.json()"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Create configuration files\n"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Create Enviorment File\n","create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs nlp-architect and the azureml-sdk. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from azureml.core.conda_dependencies import CondaDependencies \n","\n","pip = [\"azureml-defaults\", \"azureml-monitoring\", \n","       \"git+https://github.com/NervanaSystems/nlp-architect.git@absa\", \n","       \"spacy==2.0.18\"]\n","\n","myenv = CondaDependencies.create(pip_packages=pip)\n","\n","with open(\"myenv.yml\",\"w\") as f:\n","    f.write(myenv.serialize_to_string())"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Create Environment Config\n","Create a Enviorment configuration file and specify the enviroment and enviormental variables required for the application"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from azureml.core import Environment\n","deploy_env = Environment.from_conda_specification('absa_env', \"myenv.yml\")\n","deploy_env.environment_variables={'NLP_ARCHITECT_BE': 'CPU'}"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Inference Config \n","Create an inference configuration that recieves the deployment enviorment and the entry script"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from azureml.core.model import InferenceConfig\n","inference_config = InferenceConfig(environment=deploy_env,\n","                                   entry_script=\"score.py\")"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Package Model and Pull \n","Create an inference configuration that recieves the deployment enviorment and the entry script"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["package = Model.package(ws, [aspect_lex, opinion_lex], inference_config)\n","package.wait_for_creation(show_output=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["package.pull()"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Next Steps\n","\n","We now have gone through all the steps for production training of a custom open source model using the AzureML Service check out AIML50 to learn how to deploy and models and manage re-training pipelines"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
